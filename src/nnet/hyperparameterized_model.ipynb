{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# for data scaling and splitting\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "# for neural net\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# for evaluation\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 16383)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/combined_expression.csv\")\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_genes = pd.read_csv('cleaned/boruta-99-25-0.01.csv')\n",
    "selected_genes = selected_genes.values.tolist()\n",
    "selected_genes = list(itertools.chain(*selected_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving proper columns\n",
    "X = data.loc[:, selected_genes]\n",
    "y = data['classification'].values\n",
    "\n",
    "# scaling the data\n",
    "scalar = MinMaxScaler()\n",
    "x_scaled = scalar.fit_transform(X)\n",
    "\n",
    "# splitting data (20% test, 80% train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch for Input and Output Layer (no hidden layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Epochs and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='rmsprop',init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # adding layers and adding droplayers to avoid overfitting\n",
    "    hidden_layers = len(selected_genes)\n",
    "    model.add(Dense(hidden_layers, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense((hidden_layers*0.5), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense((hidden_layers*0.25), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense((hidden_layers*0.25), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compiling\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 336 candidates, totalling 1008 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1008 out of 1008 | elapsed: 31.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "513/513 [==============================] - 0s 951us/sample - loss: 0.6607 - acc: 0.6121\n",
      "Epoch 2/100\n",
      "513/513 [==============================] - 0s 89us/sample - loss: 0.6651 - acc: 0.6472\n",
      "Epoch 3/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6380 - acc: 0.6530\n",
      "Epoch 4/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.6318 - acc: 0.6413\n",
      "Epoch 5/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.6265 - acc: 0.6569\n",
      "Epoch 6/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6364 - acc: 0.6472\n",
      "Epoch 7/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.6344 - acc: 0.6452\n",
      "Epoch 8/100\n",
      "513/513 [==============================] - 0s 231us/sample - loss: 0.6254 - acc: 0.6608\n",
      "Epoch 9/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.6229 - acc: 0.6589\n",
      "Epoch 10/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.6246 - acc: 0.6569\n",
      "Epoch 11/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.6172 - acc: 0.6725\n",
      "Epoch 12/100\n",
      "513/513 [==============================] - 0s 220us/sample - loss: 0.6004 - acc: 0.6667\n",
      "Epoch 13/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.6193 - acc: 0.6628\n",
      "Epoch 14/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.6347 - acc: 0.6608\n",
      "Epoch 15/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.6210 - acc: 0.6628\n",
      "Epoch 16/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6284 - acc: 0.6725\n",
      "Epoch 17/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6226 - acc: 0.6491\n",
      "Epoch 18/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.6074 - acc: 0.6706\n",
      "Epoch 19/100\n",
      "513/513 [==============================] - 0s 117us/sample - loss: 0.6045 - acc: 0.6764\n",
      "Epoch 20/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6150 - acc: 0.6764\n",
      "Epoch 21/100\n",
      "513/513 [==============================] - 0s 102us/sample - loss: 0.6085 - acc: 0.6959\n",
      "Epoch 22/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5987 - acc: 0.6979\n",
      "Epoch 23/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6056 - acc: 0.6940\n",
      "Epoch 24/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6023 - acc: 0.6764\n",
      "Epoch 25/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5992 - acc: 0.6764\n",
      "Epoch 26/100\n",
      "513/513 [==============================] - 0s 241us/sample - loss: 0.5899 - acc: 0.6940\n",
      "Epoch 27/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5871 - acc: 0.6803\n",
      "Epoch 28/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5883 - acc: 0.6842\n",
      "Epoch 29/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.6057 - acc: 0.6881\n",
      "Epoch 30/100\n",
      "513/513 [==============================] - 0s 229us/sample - loss: 0.5970 - acc: 0.6647\n",
      "Epoch 31/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5869 - acc: 0.6920\n",
      "Epoch 32/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5979 - acc: 0.6920\n",
      "Epoch 33/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5918 - acc: 0.6959\n",
      "Epoch 34/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5983 - acc: 0.6803\n",
      "Epoch 35/100\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.5990 - acc: 0.6998\n",
      "Epoch 36/100\n",
      "513/513 [==============================] - 0s 278us/sample - loss: 0.5892 - acc: 0.6920\n",
      "Epoch 37/100\n",
      "513/513 [==============================] - 0s 282us/sample - loss: 0.5800 - acc: 0.6979\n",
      "Epoch 38/100\n",
      "513/513 [==============================] - 0s 202us/sample - loss: 0.5912 - acc: 0.6959\n",
      "Epoch 39/100\n",
      "513/513 [==============================] - 0s 377us/sample - loss: 0.5870 - acc: 0.6959\n",
      "Epoch 40/100\n",
      "513/513 [==============================] - 0s 126us/sample - loss: 0.5747 - acc: 0.7173\n",
      "Epoch 41/100\n",
      "513/513 [==============================] - 0s 220us/sample - loss: 0.5962 - acc: 0.6803\n",
      "Epoch 42/100\n",
      "513/513 [==============================] - 0s 117us/sample - loss: 0.5881 - acc: 0.6667\n",
      "Epoch 43/100\n",
      "513/513 [==============================] - 1s 1ms/sample - loss: 0.5893 - acc: 0.6940\n",
      "Epoch 44/100\n",
      "513/513 [==============================] - 0s 288us/sample - loss: 0.5856 - acc: 0.7018\n",
      "Epoch 45/100\n",
      "513/513 [==============================] - 0s 243us/sample - loss: 0.5865 - acc: 0.6979\n",
      "Epoch 46/100\n",
      "513/513 [==============================] - 0s 224us/sample - loss: 0.5871 - acc: 0.6901\n",
      "Epoch 47/100\n",
      "513/513 [==============================] - 0s 108us/sample - loss: 0.5877 - acc: 0.7037\n",
      "Epoch 48/100\n",
      "513/513 [==============================] - 0s 111us/sample - loss: 0.5703 - acc: 0.7018\n",
      "Epoch 49/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5959 - acc: 0.6959\n",
      "Epoch 50/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5826 - acc: 0.7037\n",
      "Epoch 51/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.5810 - acc: 0.7096\n",
      "Epoch 52/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.5700 - acc: 0.7076\n",
      "Epoch 53/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5994 - acc: 0.6959\n",
      "Epoch 54/100\n",
      "513/513 [==============================] - 0s 89us/sample - loss: 0.5865 - acc: 0.6940\n",
      "Epoch 55/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5777 - acc: 0.7018\n",
      "Epoch 56/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5679 - acc: 0.7173\n",
      "Epoch 57/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.5836 - acc: 0.6998\n",
      "Epoch 58/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5632 - acc: 0.7173\n",
      "Epoch 59/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5670 - acc: 0.7193\n",
      "Epoch 60/100\n",
      "513/513 [==============================] - 0s 86us/sample - loss: 0.5814 - acc: 0.7154\n",
      "Epoch 61/100\n",
      "513/513 [==============================] - 0s 220us/sample - loss: 0.5648 - acc: 0.7037\n",
      "Epoch 62/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5658 - acc: 0.7232\n",
      "Epoch 63/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5720 - acc: 0.7193\n",
      "Epoch 64/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5827 - acc: 0.7076\n",
      "Epoch 65/100\n",
      "513/513 [==============================] - 0s 214us/sample - loss: 0.5646 - acc: 0.7173\n",
      "Epoch 66/100\n",
      "513/513 [==============================] - 0s 115us/sample - loss: 0.5562 - acc: 0.7232\n",
      "Epoch 67/100\n",
      "513/513 [==============================] - 0s 110us/sample - loss: 0.5836 - acc: 0.6940\n",
      "Epoch 68/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5679 - acc: 0.6959\n",
      "Epoch 69/100\n",
      "513/513 [==============================] - 0s 111us/sample - loss: 0.5656 - acc: 0.6862\n",
      "Epoch 70/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.5711 - acc: 0.7154\n",
      "Epoch 71/100\n",
      "513/513 [==============================] - 0s 86us/sample - loss: 0.5855 - acc: 0.7154\n",
      "Epoch 72/100\n",
      "513/513 [==============================] - 0s 103us/sample - loss: 0.5707 - acc: 0.7173\n",
      "Epoch 73/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.5717 - acc: 0.7154\n",
      "Epoch 74/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5790 - acc: 0.7096\n",
      "Epoch 75/100\n",
      "513/513 [==============================] - 0s 90us/sample - loss: 0.5552 - acc: 0.7173\n",
      "Epoch 76/100\n",
      "513/513 [==============================] - 0s 115us/sample - loss: 0.5679 - acc: 0.7018\n",
      "Epoch 77/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5801 - acc: 0.7096\n",
      "Epoch 78/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.5872 - acc: 0.6998\n",
      "Epoch 79/100\n",
      "513/513 [==============================] - 0s 231us/sample - loss: 0.5502 - acc: 0.7096\n",
      "Epoch 80/100\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.5927 - acc: 0.6881\n",
      "Epoch 81/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.5515 - acc: 0.7212\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 218us/sample - loss: 0.5677 - acc: 0.6979\n",
      "Epoch 83/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5770 - acc: 0.7232\n",
      "Epoch 84/100\n",
      "513/513 [==============================] - 0s 90us/sample - loss: 0.5724 - acc: 0.7096\n",
      "Epoch 85/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5645 - acc: 0.7057\n",
      "Epoch 86/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5750 - acc: 0.7310\n",
      "Epoch 87/100\n",
      "513/513 [==============================] - 0s 92us/sample - loss: 0.5500 - acc: 0.7193\n",
      "Epoch 88/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.5717 - acc: 0.7076\n",
      "Epoch 89/100\n",
      "513/513 [==============================] - 0s 85us/sample - loss: 0.5477 - acc: 0.7096\n",
      "Epoch 90/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.5697 - acc: 0.7193\n",
      "Epoch 91/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5605 - acc: 0.7135\n",
      "Epoch 92/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5633 - acc: 0.7212\n",
      "Epoch 93/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5619 - acc: 0.7173\n",
      "Epoch 94/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5616 - acc: 0.7251\n",
      "Epoch 95/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.5670 - acc: 0.7018\n",
      "Epoch 96/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5626 - acc: 0.7349\n",
      "Epoch 97/100\n",
      "513/513 [==============================] - 0s 216us/sample - loss: 0.5605 - acc: 0.7154\n",
      "Epoch 98/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5488 - acc: 0.7096\n",
      "Epoch 99/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5598 - acc: 0.7173\n",
      "Epoch 100/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5434 - acc: 0.7135\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model)\n",
    "epochs = [50, 75, 100, 150]\n",
    "batches = [16, 32, 64, 128]\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "param_grid = dict(epochs=epochs, batch_size=batches,optimizer=optimizers,init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.754386 using {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adagrad'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 7.22039588,  9.56947915,  8.48573939,  9.27067685,  8.95816739,\n",
       "         8.88102937, 14.03832571,  7.63566136, 10.87969049,  9.99364114,\n",
       "        10.72668576,  9.70933819,  9.85021536, 15.11698103,  8.59357548,\n",
       "        11.95666877, 10.56142751, 10.84465106,  9.85390568, 10.20533554,\n",
       "        14.22944824, 10.99437936, 15.08111501, 13.09001851, 15.21255986,\n",
       "        13.07044125, 12.89606134, 19.08121816, 11.24616599, 14.71461336,\n",
       "        11.30761361, 12.28670128, 11.74078059, 11.31368788, 17.73999445,\n",
       "        10.78163417, 13.59046125, 12.35867484, 13.70239774, 12.40315477,\n",
       "        12.52348463, 18.69288786, 13.24676792, 17.92378426, 15.24838185,\n",
       "        17.705592  , 15.63943529, 15.32981396, 24.04683884, 13.98553594,\n",
       "        18.01362753, 15.96535977, 16.85418582, 15.94847647, 16.37439966,\n",
       "        25.11128616, 12.991738  , 15.1353151 , 13.29384446, 15.43697381,\n",
       "        14.84020011, 14.71931307, 23.02711749, 18.31621861, 24.135686  ,\n",
       "        22.02819777, 25.05460095, 23.17988149, 22.45404108, 35.28335619,\n",
       "        21.94853401, 28.23452067, 22.25938972, 23.46471953, 21.03998597,\n",
       "        21.58439795, 31.90487115, 18.67606942, 24.46762919, 21.17337886,\n",
       "        23.62936187, 22.94873055, 22.75427461, 32.02495917,  7.4535017 ,\n",
       "         8.6062212 ,  8.09208218,  7.55508145,  7.25777737,  7.01397236,\n",
       "         9.90174643,  5.6078999 ,  8.5342648 ,  7.91380938,  9.35099506,\n",
       "         8.60132917,  8.18653774, 12.07337832,  6.767814  , 10.1747996 ,\n",
       "         7.58717442,  7.7798183 ,  7.6151007 ,  7.39224211, 10.12290295,\n",
       "         8.10794568, 10.10674691,  8.94271413,  8.80885307,  9.21182187,\n",
       "         8.88456662, 12.63549733,  8.46145956,  9.95396312,  9.27476692,\n",
       "         9.8760554 ,  8.98669529,  9.30674911, 12.85524917,  7.60504389,\n",
       "        10.28006307,  9.30881135, 10.43065492,  9.44436749,  9.20559327,\n",
       "        14.08247304, 10.43056917, 13.32556581, 11.72755416, 13.48063294,\n",
       "        11.90679177, 12.01207209, 17.02699312, 10.68548814, 13.77802221,\n",
       "        11.94626896, 10.44703341, 10.336339  ,  9.92097441, 14.67170469,\n",
       "         9.59342408, 11.23106448, 10.83914113, 10.58890367, 10.35828177,\n",
       "        10.66546774, 14.92151825, 11.74334621, 15.56119998, 13.36021113,\n",
       "        15.16893109, 13.80845515, 13.55350971, 19.8616    , 12.0070413 ,\n",
       "        14.75711155, 13.41232006, 15.1610984 , 14.33787616, 14.52291632,\n",
       "        21.09514292, 14.16466602, 18.87464921, 15.79395692, 14.23038205,\n",
       "        14.90785074, 14.5125734 , 19.37459111,  5.65531437,  7.41502452,\n",
       "         6.10570534,  5.89753199,  5.99137886,  5.8259418 ,  8.45431662,\n",
       "         5.50943613,  6.54657563,  6.23724198,  6.40406402,  6.9494795 ,\n",
       "         6.87279431,  8.84856081,  5.30575625,  6.57275279,  6.10325821,\n",
       "         6.2649169 ,  6.37737807,  6.28090803,  8.17917021,  5.98747619,\n",
       "         7.77301582,  7.32245255,  8.04650315,  7.67532317,  8.09024167,\n",
       "        11.17208401,  7.02862302,  9.35896603,  8.33741029,  9.79668411,\n",
       "         9.37444941,  8.91325792, 11.39873282,  6.81402516,  8.89146272,\n",
       "         8.01623583,  8.74333398,  8.39196833,  8.66875243, 11.27514895,\n",
       "         7.51492755,  8.82586543,  6.04347301,  6.87918512,  7.23434424,\n",
       "        10.20350067, 24.45457896, 16.90862219, 17.38065386, 11.04049214,\n",
       "         8.08959977,  7.69944715,  8.57425602,  9.86101683,  6.02834535,\n",
       "         7.78902197,  7.39966607,  7.05380384,  6.78998685,  7.09369016,\n",
       "        10.21135449,  8.72232501, 10.11031087,  8.55868467,  9.88000806,\n",
       "         8.98300115,  8.79933929, 12.7913781 ,  8.35900529, 10.94435287,\n",
       "         9.32133174,  9.56852428,  9.5880065 , 10.0200278 , 18.49503199,\n",
       "        13.88255103, 20.09318805, 19.13328505,  9.94570343, 10.7927173 ,\n",
       "         8.49000518, 11.94000999,  3.9526879 ,  5.45700479,  4.1855282 ,\n",
       "         3.98633726,  4.01371002,  3.88498974,  5.5663542 ,  3.65399988,\n",
       "         5.0788765 ,  4.43904877,  4.54464563,  4.30870326,  4.44069322,\n",
       "         5.53467711,  3.61768683,  4.73670578,  4.93767611,  5.21769508,\n",
       "         5.21422243,  4.78035037,  6.24835666,  4.31964024,  5.76057609,\n",
       "         5.40632693,  5.80648843,  5.55204884,  5.30699873,  7.37778219,\n",
       "         4.65435823,  6.36667633,  6.21884537,  6.50483346,  6.24300933,\n",
       "         5.9048574 ,  7.75986648,  5.63065847,  6.367709  ,  6.02168997,\n",
       "         5.91496491,  5.35232671,  5.44932111,  8.77938422,  7.25267736,\n",
       "        10.20969884,  7.19365072,  6.1576759 ,  6.58865762,  7.43568659,\n",
       "         7.72424579,  4.90135566,  6.57735562,  5.86111617,  6.32564942,\n",
       "         5.91566817,  5.38800343,  7.67670433,  5.37753359,  6.16068133,\n",
       "         5.96668196,  5.1960113 ,  5.35400685,  5.8196493 ,  8.15344222,\n",
       "         7.19448447,  9.61668221,  9.7770799 , 17.69698938, 17.93288732,\n",
       "        17.89371276, 12.91339374,  9.14360325,  9.31913312,  8.95259007,\n",
       "         9.10536385,  7.51941609,  7.18430193, 10.199929  ,  7.10752821,\n",
       "         8.13327901,  7.14789502,  7.30414542,  7.62196167,  7.80048466,\n",
       "         7.83879209]),\n",
       " 'std_fit_time': array([9.22222922e-02, 8.11726011e-02, 1.06367893e-01, 5.37634588e-02,\n",
       "        3.28710832e-01, 8.02560149e-02, 1.78706116e-01, 8.59118618e-02,\n",
       "        1.77441745e-01, 3.30603681e-01, 6.25335356e-01, 6.24042575e-02,\n",
       "        2.53648655e-01, 5.17971036e-01, 4.71778196e-02, 2.74491242e-01,\n",
       "        1.28207588e-01, 2.02526671e-01, 3.14577203e-02, 3.58676775e-01,\n",
       "        2.38775317e-01, 9.51434172e-02, 7.61028801e-01, 9.71381093e-02,\n",
       "        5.96387765e-01, 5.48520990e-02, 2.84520139e-01, 9.82420059e-01,\n",
       "        2.02480236e-02, 6.95701868e-01, 9.54909376e-01, 3.07930545e-01,\n",
       "        1.52493864e-01, 1.33427706e-01, 2.39943697e-01, 9.41941276e-02,\n",
       "        2.83046291e-01, 1.85206191e-01, 1.49054784e-01, 2.48626904e-01,\n",
       "        5.30397505e-01, 8.36541460e-01, 5.76410076e-02, 1.13658602e+00,\n",
       "        7.30723776e-02, 5.14460902e-01, 5.89168681e-01, 1.95444087e-01,\n",
       "        6.99498063e-01, 4.61848441e-02, 7.15389774e-01, 2.68458300e-01,\n",
       "        8.75158403e-01, 5.80860499e-01, 4.27632040e-01, 8.49899537e-01,\n",
       "        1.09010480e+00, 1.28106688e-01, 2.65852799e-01, 5.98971656e-01,\n",
       "        5.91335997e-01, 2.45280879e-01, 2.89866001e-01, 3.65047126e-01,\n",
       "        8.06671232e-01, 6.07381210e-01, 1.30490780e-01, 9.40994053e-01,\n",
       "        1.03901752e+00, 4.39319864e-01, 6.93123640e-01, 1.73002649e+00,\n",
       "        2.03293291e+00, 1.19650401e+00, 7.93308057e-01, 2.11124785e-01,\n",
       "        5.76850645e-01, 5.69241405e-01, 1.09714879e+00, 3.09656422e-01,\n",
       "        2.23022976e-01, 4.50154542e-01, 6.60006136e-01, 5.81189556e-02,\n",
       "        3.42641603e-01, 4.84772334e-02, 5.24293236e-01, 6.56472267e-02,\n",
       "        1.34104367e-01, 3.83680004e-01, 4.29568132e-01, 7.31919657e-03,\n",
       "        2.37548352e-01, 3.61256622e-01, 5.79452842e-01, 8.11873327e-01,\n",
       "        5.42230094e-01, 3.61525894e-01, 5.55718659e-01, 5.13475567e-01,\n",
       "        9.17536867e-01, 8.61457478e-01, 8.37547718e-02, 3.53994186e-01,\n",
       "        4.00670857e-01, 8.38108518e-01, 1.03810035e+00, 4.28480785e-01,\n",
       "        2.80908561e-01, 2.67526266e-01, 2.42315748e-01, 3.21506720e-01,\n",
       "        5.64648909e-01, 3.31169903e-01, 1.11616633e-01, 1.13631077e+00,\n",
       "        6.31390820e-02, 8.89694006e-01, 3.43654542e-01, 4.36451713e-01,\n",
       "        8.66683823e-01, 1.91802342e-01, 4.07935193e-01, 5.99077492e-01,\n",
       "        3.39566185e-01, 1.24849480e-01, 5.27743433e-01, 4.03005017e-01,\n",
       "        3.96584778e-01, 6.44602618e-01, 7.87308928e-01, 4.38821702e-01,\n",
       "        3.70158227e-01, 2.48809514e-01, 9.30066718e-01, 2.31001513e-01,\n",
       "        1.00871747e+00, 1.99115028e-01, 6.12043721e-01, 8.20407273e-01,\n",
       "        7.64414057e-01, 3.82216260e-01, 9.67449012e-01, 1.96138153e-01,\n",
       "        5.56156801e-02, 2.73745254e-01, 1.69382263e-01, 2.50029445e-01,\n",
       "        8.88449554e-01, 3.07567677e-01, 4.54399573e-01, 3.24543981e-01,\n",
       "        2.52457768e-01, 2.18628288e+00, 2.04562638e-01, 3.89748447e-01,\n",
       "        3.37040090e-01, 7.70287096e-01, 5.25599843e-01, 4.54252249e-01,\n",
       "        9.04220582e-01, 3.08459583e-01, 9.42992445e-01, 4.51809642e-01,\n",
       "        6.22637183e-01, 6.77800823e-01, 6.86794584e-01, 1.03485189e+00,\n",
       "        3.33578825e-01, 4.80583587e-01, 1.82325720e-01, 2.62632885e-01,\n",
       "        1.68574449e-01, 8.01942145e-02, 1.53457397e-01, 1.20252844e-01,\n",
       "        1.18533176e-01, 3.49672118e-01, 1.91703864e-01, 1.91112166e-01,\n",
       "        1.94915711e-01, 2.73572429e-01, 4.78782326e-03, 4.14604072e-01,\n",
       "        8.96941921e-02, 1.08854870e-01, 4.56352030e-01, 3.01190957e-01,\n",
       "        3.15325126e-01, 2.38558602e-01, 1.09496136e-01, 2.43200186e-01,\n",
       "        4.62281251e-01, 3.18769889e-01, 1.27509463e-01, 1.63267428e-01,\n",
       "        3.25016473e-01, 2.93389910e-01, 6.72417671e-01, 2.28563515e-01,\n",
       "        2.12822933e-01, 4.48056770e-01, 1.52475451e-01, 4.36921978e-01,\n",
       "        6.15050399e-01, 2.88318673e-01, 1.68077325e-01, 4.64827966e-02,\n",
       "        3.34343453e-01, 1.46497526e+00, 1.40994783e+00, 1.65714068e+00,\n",
       "        9.96389351e-02, 8.70659348e-01, 8.02218258e-01, 2.95628819e+00,\n",
       "        2.05997533e+00, 1.57423371e+00, 2.74205263e+00, 8.99297076e-01,\n",
       "        1.06901898e+00, 1.07298809e+00, 7.89211281e-01, 6.82568455e-02,\n",
       "        1.75913548e-01, 4.63433271e-01, 5.14870898e-02, 6.86475332e-01,\n",
       "        1.46766238e-01, 6.54487141e-01, 7.20700510e-01, 1.04739400e-01,\n",
       "        6.44562520e-01, 3.73603582e-01, 2.52034557e-01, 3.74170114e-01,\n",
       "        1.39113832e-01, 4.88158630e-01, 5.24942562e-01, 1.07473460e+00,\n",
       "        1.13288535e-01, 4.26267958e-01, 1.77764834e-01, 3.05138580e-01,\n",
       "        3.01986822e+00, 2.27175496e+00, 5.00070546e+00, 1.73860864e+00,\n",
       "        1.66552151e-01, 5.16038142e-01, 8.49341638e-01, 8.24428347e-01,\n",
       "        5.00596728e-01, 6.67772076e-01, 2.10217186e-01, 1.05621043e-01,\n",
       "        2.43139068e-01, 1.30176094e-01, 1.50148034e-01, 1.55845046e-01,\n",
       "        2.19757670e-01, 3.31889241e-01, 3.68265247e-01, 1.63435788e-01,\n",
       "        9.72036609e-02, 4.73076384e-01, 2.11594221e-01, 1.62317169e-01,\n",
       "        2.67636353e-01, 3.44599417e-01, 3.30118591e-01, 2.19332699e-01,\n",
       "        1.22999565e-01, 2.08710749e-01, 1.22143677e-01, 4.20751348e-01,\n",
       "        2.03771898e-01, 4.83409442e-01, 1.36045347e-01, 1.89053864e-01,\n",
       "        3.22135840e-01, 2.17139950e-01, 1.40585464e-01, 1.92846868e-01,\n",
       "        4.88937306e-01, 1.51832670e-01, 1.88354334e-01, 4.94377709e-01,\n",
       "        2.79372039e-01, 2.71523152e-01, 1.83063304e-01, 3.00931640e-01,\n",
       "        1.38586965e-01, 1.18169923e+00, 1.23464568e+00, 3.49440508e-01,\n",
       "        1.20962787e+00, 6.04662036e-01, 9.69585550e-01, 4.13404817e-01,\n",
       "        9.59843630e-01, 2.92259078e-01, 1.24952307e+00, 1.76320928e-01,\n",
       "        1.27827256e+00, 8.63184104e-01, 3.87361555e-02, 6.94302610e-01,\n",
       "        5.33405945e-01, 8.28136823e-01, 1.70764059e-01, 3.58852987e-01,\n",
       "        9.04444822e-02, 4.26290556e-02, 3.45192398e-01, 4.47438557e-01,\n",
       "        8.01429470e-02, 6.88511263e-01, 3.53234277e+00, 6.07610183e-01,\n",
       "        8.12251418e-01, 8.07724296e-01, 2.47467577e-01, 3.13742901e-01,\n",
       "        1.60717486e-01, 2.58016591e-01, 4.62911851e-01, 1.24897013e-01,\n",
       "        5.85530791e-02, 3.46986378e-01, 6.15415803e-01, 1.85574996e-01,\n",
       "        9.92608177e-02, 3.42498847e-01, 2.72821775e-02, 9.97452677e-01]),\n",
       " 'mean_score_time': array([0.7152249 , 0.38194799, 0.5261395 , 0.49185642, 0.48566238,\n",
       "        0.69787868, 0.83917483, 0.60510715, 0.69134029, 0.76417152,\n",
       "        0.66237489, 0.59486183, 0.7490228 , 0.90684462, 0.90685487,\n",
       "        0.73470211, 0.79895385, 0.92121625, 0.75571068, 0.79470046,\n",
       "        0.76921741, 0.81164463, 0.95237835, 0.80374352, 0.95650419,\n",
       "        0.94265683, 0.81162429, 0.94446087, 1.04357998, 0.79844777,\n",
       "        0.70301557, 0.42528296, 0.56790352, 0.58869982, 0.552097  ,\n",
       "        0.67117651, 0.72029424, 0.64615989, 0.67759514, 0.64372492,\n",
       "        0.6815292 , 0.72048585, 0.76632587, 0.87517985, 0.69335977,\n",
       "        0.84106088, 0.88130244, 0.72853287, 0.91053836, 1.10545842,\n",
       "        0.91317225, 0.89667296, 0.77754068, 0.93934504, 0.84352692,\n",
       "        1.00392564, 0.51906069, 0.50966748, 0.57599195, 0.54801925,\n",
       "        0.60479069, 0.68699408, 0.86057115, 0.72066728, 0.90704282,\n",
       "        0.87746779, 0.84632842, 0.96399736, 0.93821891, 1.19624519,\n",
       "        1.30160435, 0.85725745, 0.67990367, 0.75646758, 0.81807661,\n",
       "        0.81105121, 0.74774241, 0.512139  , 0.71534204, 0.61840916,\n",
       "        0.7394352 , 1.02026033, 0.82302252, 0.526443  , 0.63030616,\n",
       "        0.91397063, 0.77794162, 0.8863341 , 0.5599645 , 0.76393032,\n",
       "        0.86518669, 0.6805009 , 0.86869597, 0.77920008, 1.10350267,\n",
       "        0.88364744, 0.93233546, 1.00521564, 1.10614705, 0.89312434,\n",
       "        0.66059367, 0.85798971, 0.72897903, 0.6119295 , 0.89659532,\n",
       "        0.79726227, 0.72650957, 0.52438037, 0.62884935, 0.46552285,\n",
       "        0.60323199, 0.64194163, 0.83897368, 0.73036122, 0.60635885,\n",
       "        0.84186769, 0.79861975, 0.67027362, 0.66598884, 0.85688806,\n",
       "        0.8473355 , 0.8677551 , 0.79462115, 0.8450648 , 0.7574362 ,\n",
       "        1.08237807, 0.95407716, 0.9967405 , 1.05823406, 0.93119526,\n",
       "        1.08645455, 0.99195226, 1.17812483, 1.09636243, 0.98654278,\n",
       "        0.94591578, 0.57600347, 0.79076052, 0.75362984, 0.63518691,\n",
       "        0.66338174, 0.60168338, 0.73916626, 0.62235792, 0.59889682,\n",
       "        0.61216251, 0.72677525, 0.56368748, 0.66912754, 0.62896419,\n",
       "        0.61264014, 0.71733443, 0.55957206, 0.79070719, 0.9202172 ,\n",
       "        0.64107879, 1.04135919, 0.71537042, 1.00184711, 1.0456442 ,\n",
       "        1.05512921, 1.02529772, 1.02690578, 0.66626811, 0.6184984 ,\n",
       "        0.89723802, 0.80924344, 0.7309231 , 0.55965074, 0.60292252,\n",
       "        0.54853328, 0.52384313, 0.45330413, 0.5101831 , 0.64108253,\n",
       "        0.67717473, 0.83777984, 0.75367816, 0.69034394, 0.74993126,\n",
       "        0.53689218, 0.66874218, 0.76648156, 0.58005937, 0.77103178,\n",
       "        0.6747191 , 0.65319888, 0.61631211, 0.75972795, 0.68247422,\n",
       "        0.79902395, 0.73408198, 0.79356607, 0.80148729, 0.93763741,\n",
       "        1.05796663, 0.82969586, 1.13252656, 0.88333662, 1.09278297,\n",
       "        0.82400076, 0.93212255, 0.93527794, 0.9703385 , 0.99965906,\n",
       "        1.1072998 , 1.10623813, 0.92263786, 0.92381175, 0.95539355,\n",
       "        0.86577733, 0.71950221, 0.38628554, 0.6306742 , 0.58876729,\n",
       "        1.47883574, 1.84218335, 1.15816363, 0.57150769, 0.52104584,\n",
       "        0.60075116, 0.5466489 , 0.54468187, 0.56034064, 0.57132451,\n",
       "        0.55201173, 0.53497052, 0.53882424, 0.64636318, 0.71266158,\n",
       "        0.65065392, 0.59400956, 0.67933591, 0.6526622 , 0.67799115,\n",
       "        0.57265099, 0.66666818, 0.81732885, 0.92366807, 0.78746335,\n",
       "        0.69865362, 0.72131284, 0.79301421, 0.77732245, 2.18823099,\n",
       "        2.26218724, 1.14767464, 0.8892835 , 0.35300199, 0.73228057,\n",
       "        0.64402397, 0.53366828, 0.46632036, 0.55669729, 0.40549572,\n",
       "        0.38764421, 0.38295205, 0.38001895, 0.45700232, 0.52134228,\n",
       "        0.60515078, 0.41397619, 0.47798602, 0.47927586, 0.44531552,\n",
       "        0.50032973, 0.5916547 , 0.53878856, 0.7016472 , 0.56666756,\n",
       "        0.54332272, 0.55684074, 0.67533541, 0.5280091 , 0.71680164,\n",
       "        0.73065591, 0.64056865, 0.64628609, 0.58667016, 0.69635129,\n",
       "        0.74899197, 0.65764904, 0.74564155, 0.8015372 , 0.6923209 ,\n",
       "        0.86868334, 0.88516108, 0.83136392, 0.83865062, 0.72632448,\n",
       "        0.74552051, 0.74734251, 0.83300503, 1.6496559 , 1.46903865,\n",
       "        0.9413356 , 0.62398545, 0.56869014, 0.83667938, 0.93167702,\n",
       "        0.54065768, 0.42416151, 0.59499844, 0.42856916, 0.6309948 ,\n",
       "        0.54634666, 0.50551828, 0.47398671, 0.46046813, 0.44235706,\n",
       "        0.42632707, 0.46532853, 0.40134875, 0.75737548, 0.43058435,\n",
       "        0.58835014, 1.05197557, 0.82798648, 1.42389345, 0.93302321,\n",
       "        0.85803994, 0.64959661, 0.74101853, 0.69680309, 0.78374465,\n",
       "        0.60405143, 0.55518158, 0.71741772, 0.73836732, 0.6279877 ,\n",
       "        0.63796107, 0.66788252, 0.64593951, 0.730714  , 0.49002401,\n",
       "        0.29627188]),\n",
       " 'std_score_time': array([0.02340117, 0.03365499, 0.13418482, 0.02881238, 0.04180923,\n",
       "        0.13542695, 0.099024  , 0.04458312, 0.01019333, 0.11151196,\n",
       "        0.07302029, 0.00176248, 0.07408135, 0.03882327, 0.06059339,\n",
       "        0.03529219, 0.05191363, 0.07600603, 0.08249304, 0.17491009,\n",
       "        0.11615058, 0.05542558, 0.120444  , 0.04881811, 0.25374028,\n",
       "        0.2621787 , 0.0460488 , 0.12896603, 0.01415393, 0.06809597,\n",
       "        0.26454588, 0.00373591, 0.08628598, 0.01235556, 0.04690576,\n",
       "        0.07297721, 0.08962323, 0.08450727, 0.077241  , 0.01924184,\n",
       "        0.07609804, 0.23624315, 0.07427388, 0.15952925, 0.04911648,\n",
       "        0.22979834, 0.12325824, 0.03013089, 0.13290799, 0.15174885,\n",
       "        0.00719138, 0.05106661, 0.03125265, 0.06742972, 0.29109958,\n",
       "        0.09366084, 0.10761056, 0.05723716, 0.09500505, 0.00331236,\n",
       "        0.10918283, 0.08976669, 0.24346836, 0.04397459, 0.10936124,\n",
       "        0.05824698, 0.10406721, 0.11495023, 0.23972696, 0.23513265,\n",
       "        0.16877962, 0.09122688, 0.08338839, 0.02598164, 0.22366122,\n",
       "        0.22778707, 0.19213943, 0.08536125, 0.23973109, 0.02592173,\n",
       "        0.01985029, 0.18935059, 0.09498074, 0.01809779, 0.09654561,\n",
       "        0.12981578, 0.09856558, 0.06074917, 0.01167066, 0.17760736,\n",
       "        0.25665529, 0.02962245, 0.21388134, 0.09924453, 0.10043482,\n",
       "        0.03010488, 0.0505393 , 0.24594886, 0.11061202, 0.0732964 ,\n",
       "        0.23202393, 0.27173807, 0.14222289, 0.22784242, 0.23142154,\n",
       "        0.47170629, 0.29268296, 0.05158149, 0.10981512, 0.11293748,\n",
       "        0.14994786, 0.10444559, 0.25815732, 0.11922265, 0.14307468,\n",
       "        0.19264948, 0.05651405, 0.12017256, 0.0682524 , 0.06479382,\n",
       "        0.13949039, 0.12603386, 0.08695121, 0.10255271, 0.09757993,\n",
       "        0.29446817, 0.0287422 , 0.16129492, 0.10184698, 0.07476639,\n",
       "        0.08168999, 0.07510642, 0.07107771, 0.06750122, 0.12418589,\n",
       "        0.10056021, 0.22579892, 0.26156455, 0.146445  , 0.18395639,\n",
       "        0.22052594, 0.08725623, 0.18726767, 0.14174378, 0.08189363,\n",
       "        0.2828545 , 0.14821683, 0.27096718, 0.11307971, 0.12381052,\n",
       "        0.18560617, 0.09787807, 0.16226035, 0.26003459, 0.02414667,\n",
       "        0.10765278, 0.15082858, 0.06005876, 0.15222737, 0.15729522,\n",
       "        0.08608869, 0.07212649, 0.1741396 , 0.0338312 , 0.13759323,\n",
       "        0.33888484, 0.221091  , 0.18116498, 0.06667725, 0.17521547,\n",
       "        0.04292279, 0.07966939, 0.09817119, 0.06082747, 0.10222376,\n",
       "        0.07497746, 0.00690797, 0.09367152, 0.20959261, 0.04025139,\n",
       "        0.11800895, 0.04644216, 0.04318738, 0.04325324, 0.05337769,\n",
       "        0.10617603, 0.03973355, 0.00804675, 0.02009832, 0.02211807,\n",
       "        0.06919632, 0.04930616, 0.04152877, 0.00671446, 0.08826967,\n",
       "        0.15527685, 0.11459775, 0.08392153, 0.08937946, 0.0173466 ,\n",
       "        0.08455111, 0.06643857, 0.05129021, 0.1885436 , 0.04234225,\n",
       "        0.05151133, 0.03142766, 0.1793124 , 0.43606245, 0.44830654,\n",
       "        0.35893078, 0.23607393, 0.02868099, 0.23552719, 0.23392155,\n",
       "        0.42769369, 0.31306684, 0.08437733, 0.0959029 , 0.01881492,\n",
       "        0.09542077, 0.08905   , 0.14110562, 0.11156802, 0.10101064,\n",
       "        0.11555737, 0.0257325 , 0.06847677, 0.04500339, 0.09425609,\n",
       "        0.052317  , 0.01525615, 0.11389651, 0.03884531, 0.07081442,\n",
       "        0.08295549, 0.0565595 , 0.14714821, 0.16749229, 0.0914213 ,\n",
       "        0.03222241, 0.13557145, 0.0528905 , 0.03368223, 0.42160417,\n",
       "        0.53729891, 0.61829872, 0.10496399, 0.01218138, 0.29724039,\n",
       "        0.25546755, 0.17417771, 0.03564448, 0.16392541, 0.04785165,\n",
       "        0.05414128, 0.01746608, 0.02254641, 0.05924239, 0.09632148,\n",
       "        0.0508945 , 0.05115763, 0.07871246, 0.04261502, 0.04392954,\n",
       "        0.10933711, 0.04534466, 0.02827194, 0.03766545, 0.05510315,\n",
       "        0.03308281, 0.0758071 , 0.03852528, 0.02698735, 0.05026232,\n",
       "        0.11079013, 0.04623647, 0.09064611, 0.10563941, 0.03930368,\n",
       "        0.09231161, 0.08374064, 0.13467648, 0.09705027, 0.08462536,\n",
       "        0.01157407, 0.07577621, 0.01216358, 0.13367027, 0.05452527,\n",
       "        0.02661779, 0.08568033, 0.03580908, 0.20918515, 0.20192704,\n",
       "        0.05940316, 0.19645591, 0.34601879, 0.28205826, 0.00953372,\n",
       "        0.21483116, 0.04504009, 0.26616143, 0.03392611, 0.25675144,\n",
       "        0.20576365, 0.07887766, 0.08764004, 0.11556219, 0.06703163,\n",
       "        0.10124828, 0.07308369, 0.01318327, 0.16689803, 0.06197838,\n",
       "        0.10232462, 0.17836469, 0.13644579, 0.61206712, 0.12607028,\n",
       "        0.07739307, 0.09206568, 0.11115272, 0.05885713, 0.05827224,\n",
       "        0.10231399, 0.13773504, 0.17103876, 0.08438636, 0.08933891,\n",
       "        0.06355159, 0.05089502, 0.0394446 , 0.15076575, 0.0885939 ,\n",
       "        0.03934935]),\n",
       " 'param_batch_size': masked_array(data=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 75, 75, 75, 75, 75, 75, 75,\n",
       "                    75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "                    75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "                    75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "                    50, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75,\n",
       "                    75, 75, 75, 75, 75, 75, 75, 75, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150,\n",
       "                    150, 150, 150, 150, 150],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_init': masked_array(data=['glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'glorot_uniform', 'glorot_uniform',\n",
       "                    'glorot_uniform', 'normal', 'normal', 'normal',\n",
       "                    'normal', 'normal', 'normal', 'normal', 'uniform',\n",
       "                    'uniform', 'uniform', 'uniform', 'uniform', 'uniform',\n",
       "                    'uniform'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_optimizer': masked_array(data=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam',\n",
       "                    'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta',\n",
       "                    'Adam', 'Adamax', 'Nadam', 'SGD', 'RMSprop', 'Adagrad',\n",
       "                    'Adadelta', 'Adam', 'Adamax', 'Nadam', 'SGD',\n",
       "                    'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax',\n",
       "                    'Nadam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 16,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 16, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 32,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 32, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 64,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 64, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 50,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 50, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 75,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 75, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'normal',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 100,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adam'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'normal',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'normal', 'optimizer': 'Nadam'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'uniform', 'optimizer': 'SGD'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adagrad'},\n",
       "  {'batch_size': 128,\n",
       "   'epochs': 150,\n",
       "   'init': 'uniform',\n",
       "   'optimizer': 'Adadelta'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adam'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Adamax'},\n",
       "  {'batch_size': 128, 'epochs': 150, 'init': 'uniform', 'optimizer': 'Nadam'}],\n",
       " 'split0_test_score': array([0.76608187, 0.74269009, 0.75438595, 0.66666669, 0.72514617,\n",
       "        0.73099416, 0.6608187 , 0.74269009, 0.7368421 , 0.72514617,\n",
       "        0.66666669, 0.70760232, 0.70760232, 0.69005847, 0.72514617,\n",
       "        0.74269009, 0.72514617, 0.66666669, 0.73099416, 0.71345031,\n",
       "        0.69590646, 0.70760232, 0.75438595, 0.7368421 , 0.66666669,\n",
       "        0.72514617, 0.72514617, 0.70175439, 0.74853802, 0.71929824,\n",
       "        0.74853802, 0.66666669, 0.69005847, 0.72514617, 0.69005847,\n",
       "        0.74853802, 0.75438595, 0.73099416, 0.66666669, 0.72514617,\n",
       "        0.72514617, 0.71929824, 0.69590646, 0.71929824, 0.7719298 ,\n",
       "        0.66666669, 0.68421054, 0.73099416, 0.67251462, 0.73099416,\n",
       "        0.69590646, 0.71929824, 0.66666669, 0.69005847, 0.73099416,\n",
       "        0.69590646, 0.69590646, 0.70175439, 0.75438595, 0.66666669,\n",
       "        0.65497077, 0.7368421 , 0.67251462, 0.68421054, 0.69005847,\n",
       "        0.72514617, 0.66666669, 0.7368421 , 0.68421054, 0.7368421 ,\n",
       "        0.71929824, 0.71345031, 0.74853802, 0.66666669, 0.67836255,\n",
       "        0.71345031, 0.63157892, 0.62573099, 0.68421054, 0.7368421 ,\n",
       "        0.66666669, 0.73099416, 0.71345031, 0.69590646, 0.71929824,\n",
       "        0.70760232, 0.72514617, 0.65497077, 0.65497077, 0.73099416,\n",
       "        0.67836255, 0.7368421 , 0.64912283, 0.74269009, 0.42105263,\n",
       "        0.72514617, 0.74853802, 0.70175439, 0.71345031, 0.61988306,\n",
       "        0.73099416, 0.64912283, 0.71345031, 0.72514617, 0.64327484,\n",
       "        0.71345031, 0.6608187 , 0.74853802, 0.66666669, 0.71929824,\n",
       "        0.74269009, 0.70175439, 0.7368421 , 0.71345031, 0.72514617,\n",
       "        0.66666669, 0.72514617, 0.70760232, 0.70175439, 0.73099416,\n",
       "        0.67836255, 0.76608187, 0.66666669, 0.70760232, 0.71929824,\n",
       "        0.73099416, 0.74269009, 0.72514617, 0.71345031, 0.66666669,\n",
       "        0.71929824, 0.72514617, 0.70175439, 0.71345031, 0.70175439,\n",
       "        0.71345031, 0.66666669, 0.7368421 , 0.71345031, 0.7368421 ,\n",
       "        0.7368421 , 0.74853802, 0.72514617, 0.66666669, 0.6608187 ,\n",
       "        0.69005847, 0.70175439, 0.71345031, 0.68421054, 0.74269009,\n",
       "        0.66666669, 0.69590646, 0.71929824, 0.73099416, 0.73099416,\n",
       "        0.74853802, 0.7368421 , 0.66666669, 0.71929824, 0.73099416,\n",
       "        0.74269009, 0.71345031, 0.71929824, 0.73099416, 0.66666669,\n",
       "        0.71929824, 0.70760232, 0.6608187 , 0.69590646, 0.67251462,\n",
       "        0.74853802, 0.4269006 , 0.73099416, 0.72514617, 0.71345031,\n",
       "        0.70175439, 0.68421054, 0.7368421 , 0.64912283, 0.68421054,\n",
       "        0.71929824, 0.74269009, 0.69590646, 0.7368421 , 0.74853802,\n",
       "        0.66666669, 0.74269009, 0.72514617, 0.56140351, 0.73099416,\n",
       "        0.69005847, 0.7368421 , 0.66666669, 0.71345031, 0.73099416,\n",
       "        0.71929824, 0.70175439, 0.59649122, 0.71929824, 0.64912283,\n",
       "        0.72514617, 0.7368421 , 0.70760232, 0.72514617, 0.70175439,\n",
       "        0.72514617, 0.66666669, 0.70760232, 0.7368421 , 0.70175439,\n",
       "        0.71929824, 0.68421054, 0.74269009, 0.66666669, 0.7368421 ,\n",
       "        0.68421054, 0.72514617, 0.72514617, 0.70760232, 0.76023394,\n",
       "        0.66666669, 0.74269009, 0.70760232, 0.72514617, 0.71929824,\n",
       "        0.73099416, 0.72514617, 0.66666669, 0.71929824, 0.74269009,\n",
       "        0.65497077, 0.7368421 , 0.72514617, 0.7368421 , 0.66666669,\n",
       "        0.70760232, 0.74269009, 0.63742691, 0.73099416, 0.70175439,\n",
       "        0.73099416, 0.66666669, 0.69005847, 0.70175439, 0.73099416,\n",
       "        0.73099416, 0.70760232, 0.7368421 , 0.66666669, 0.72514617,\n",
       "        0.73099416, 0.71345031, 0.67251462, 0.68421054, 0.73099416,\n",
       "        0.68421054, 0.72514617, 0.7368421 , 0.68421054, 0.67251462,\n",
       "        0.70175439, 0.71929824, 0.33333334, 0.69590646, 0.74853802,\n",
       "        0.7368421 , 0.66666669, 0.7368421 , 0.74853802, 0.6608187 ,\n",
       "        0.71345031, 0.74853802, 0.75438595, 0.67251462, 0.75438595,\n",
       "        0.74269009, 0.66666669, 0.71929824, 0.71929824, 0.69005847,\n",
       "        0.68421054, 0.72514617, 0.73099416, 0.33333334, 0.71929824,\n",
       "        0.71345031, 0.71929824, 0.69005847, 0.70175439, 0.7368421 ,\n",
       "        0.6608187 , 0.71929824, 0.7368421 , 0.74269009, 0.69590646,\n",
       "        0.74269009, 0.74269009, 0.66666669, 0.70175439, 0.73099416,\n",
       "        0.70760232, 0.69590646, 0.71345031, 0.73099416, 0.66666669,\n",
       "        0.71929824, 0.71345031, 0.63742691, 0.69590646, 0.71345031,\n",
       "        0.72514617, 0.66666669, 0.71929824, 0.75438595, 0.71345031,\n",
       "        0.73099416, 0.72514617, 0.74269009, 0.66666669, 0.71345031,\n",
       "        0.73099416, 0.70760232, 0.72514617, 0.71929824, 0.72514617,\n",
       "        0.66666669, 0.7368421 , 0.73099416, 0.70760232, 0.71929824,\n",
       "        0.68421054, 0.74853802, 0.66666669, 0.7368421 , 0.73099416,\n",
       "        0.72514617]),\n",
       " 'split1_test_score': array([0.74269009, 0.71345031, 0.74269009, 0.62573099, 0.70760232,\n",
       "        0.73099416, 0.73099416, 0.70175439, 0.71345031, 0.73099416,\n",
       "        0.62573099, 0.72514617, 0.74269009, 0.63157892, 0.72514617,\n",
       "        0.66666669, 0.70760232, 0.62573099, 0.70175439, 0.72514617,\n",
       "        0.7368421 , 0.71345031, 0.70175439, 0.71929824, 0.62573099,\n",
       "        0.75438595, 0.71345031, 0.72514617, 0.70175439, 0.64912283,\n",
       "        0.73099416, 0.62573099, 0.69590646, 0.7368421 , 0.72514617,\n",
       "        0.70760232, 0.67836255, 0.7368421 , 0.62573099, 0.70760232,\n",
       "        0.73099416, 0.67836255, 0.70760232, 0.69005847, 0.72514617,\n",
       "        0.62573099, 0.70175439, 0.70760232, 0.71345031, 0.7368421 ,\n",
       "        0.74853802, 0.73099416, 0.62573099, 0.71345031, 0.69005847,\n",
       "        0.71345031, 0.71929824, 0.70760232, 0.7368421 , 0.62573099,\n",
       "        0.70760232, 0.72514617, 0.71345031, 0.71929824, 0.70175439,\n",
       "        0.73099416, 0.62573099, 0.72514617, 0.71929824, 0.69590646,\n",
       "        0.70760232, 0.69590646, 0.73099416, 0.62573099, 0.71929824,\n",
       "        0.69590646, 0.69005847, 0.70760232, 0.68421054, 0.71345031,\n",
       "        0.62573099, 0.7368421 , 0.69590646, 0.70175439, 0.67836255,\n",
       "        0.68421054, 0.7368421 , 0.62573099, 0.70760232, 0.7368421 ,\n",
       "        0.71929824, 0.71929824, 0.67836255, 0.69005847, 0.62573099,\n",
       "        0.71929824, 0.71929824, 0.69590646, 0.73099416, 0.71345031,\n",
       "        0.73099416, 0.62573099, 0.7368421 , 0.71929824, 0.71929824,\n",
       "        0.72514617, 0.66666669, 0.74853802, 0.62573099, 0.71929824,\n",
       "        0.73099416, 0.75438595, 0.71345031, 0.70175439, 0.71345031,\n",
       "        0.62573099, 0.73099416, 0.71345031, 0.71929824, 0.71929824,\n",
       "        0.70760232, 0.70760232, 0.62573099, 0.71345031, 0.73099416,\n",
       "        0.70175439, 0.72514617, 0.71345031, 0.73099416, 0.62573099,\n",
       "        0.70175439, 0.70175439, 0.70175439, 0.70760232, 0.70175439,\n",
       "        0.74269009, 0.62573099, 0.70175439, 0.66666669, 0.72514617,\n",
       "        0.7368421 , 0.6608187 , 0.72514617, 0.62573099, 0.70760232,\n",
       "        0.69590646, 0.71345031, 0.70760232, 0.72514617, 0.76608187,\n",
       "        0.62573099, 0.71929824, 0.7368421 , 0.72514617, 0.69005847,\n",
       "        0.69590646, 0.71345031, 0.62573099, 0.71345031, 0.71345031,\n",
       "        0.71345031, 0.71345031, 0.7368421 , 0.7368421 , 0.62573099,\n",
       "        0.72514617, 0.70760232, 0.69005847, 0.64327484, 0.74269009,\n",
       "        0.70760232, 0.65497077, 0.74269009, 0.71345031, 0.7368421 ,\n",
       "        0.63157892, 0.69005847, 0.70760232, 0.62573099, 0.71345031,\n",
       "        0.73099416, 0.71929824, 0.63742691, 0.70175439, 0.69590646,\n",
       "        0.62573099, 0.72514617, 0.71345031, 0.71929824, 0.67836255,\n",
       "        0.74269009, 0.71345031, 0.62573099, 0.71929824, 0.73099416,\n",
       "        0.75438595, 0.64912283, 0.74269009, 0.71929824, 0.62573099,\n",
       "        0.71929824, 0.75438595, 0.67836255, 0.67251462, 0.73099416,\n",
       "        0.71345031, 0.62573099, 0.71345031, 0.71345031, 0.70760232,\n",
       "        0.69005847, 0.70760232, 0.7368421 , 0.63157892, 0.67251462,\n",
       "        0.7368421 , 0.69590646, 0.70760232, 0.70175439, 0.72514617,\n",
       "        0.62573099, 0.71929824, 0.73099416, 0.72514617, 0.6608187 ,\n",
       "        0.71345031, 0.72514617, 0.62573099, 0.71345031, 0.75438595,\n",
       "        0.71929824, 0.70760232, 0.71345031, 0.73099416, 0.62573099,\n",
       "        0.72514617, 0.69005847, 0.57894737, 0.71929824, 0.61403507,\n",
       "        0.72514617, 0.62573099, 0.73099416, 0.73099416, 0.71929824,\n",
       "        0.73099416, 0.72514617, 0.73099416, 0.62573099, 0.73099416,\n",
       "        0.7368421 , 0.74269009, 0.62573099, 0.77777779, 0.72514617,\n",
       "        0.62573099, 0.74269009, 0.74269009, 0.69590646, 0.63742691,\n",
       "        0.6608187 , 0.7368421 , 0.62573099, 0.71345031, 0.70760232,\n",
       "        0.70760232, 0.62573099, 0.70175439, 0.72514617, 0.59649122,\n",
       "        0.71929824, 0.72514617, 0.71929824, 0.62573099, 0.74269009,\n",
       "        0.7368421 , 0.62573099, 0.71929824, 0.72514617, 0.6608187 ,\n",
       "        0.63742691, 0.69005847, 0.72514617, 0.4269006 , 0.7368421 ,\n",
       "        0.76023394, 0.71345031, 0.61988306, 0.70760232, 0.71929824,\n",
       "        0.4269006 , 0.71345031, 0.7368421 , 0.72514617, 0.65497077,\n",
       "        0.67836255, 0.74853802, 0.45029241, 0.73099416, 0.71929824,\n",
       "        0.68421054, 0.64327484, 0.7368421 , 0.70175439, 0.59649122,\n",
       "        0.71929824, 0.71929824, 0.70175439, 0.67251462, 0.70175439,\n",
       "        0.72514617, 0.54385966, 0.71345031, 0.71345031, 0.71345031,\n",
       "        0.69590646, 0.66666669, 0.71345031, 0.62573099, 0.70175439,\n",
       "        0.69590646, 0.68421054, 0.6608187 , 0.68421054, 0.74853802,\n",
       "        0.63157892, 0.7368421 , 0.70760232, 0.7368421 , 0.67251462,\n",
       "        0.74269009, 0.74269009, 0.62573099, 0.68421054, 0.71929824,\n",
       "        0.70760232]),\n",
       " 'split2_test_score': array([0.68421054, 0.68421054, 0.7368421 , 0.69005847, 0.69590646,\n",
       "        0.72514617, 0.71929824, 0.61988306, 0.71929824, 0.71929824,\n",
       "        0.69005847, 0.67836255, 0.70175439, 0.57894737, 0.70760232,\n",
       "        0.71929824, 0.71929824, 0.69005847, 0.62573099, 0.71345031,\n",
       "        0.67251462, 0.72514617, 0.65497077, 0.72514617, 0.69005847,\n",
       "        0.62573099, 0.70175439, 0.6608187 , 0.71929824, 0.69590646,\n",
       "        0.72514617, 0.69005847, 0.69005847, 0.69590646, 0.68421054,\n",
       "        0.69005847, 0.69590646, 0.73099416, 0.69005847, 0.68421054,\n",
       "        0.70175439, 0.70175439, 0.72514617, 0.66666669, 0.71345031,\n",
       "        0.69005847, 0.70760232, 0.71345031, 0.68421054, 0.70760232,\n",
       "        0.70760232, 0.72514617, 0.69005847, 0.69005847, 0.70175439,\n",
       "        0.66666669, 0.71345031, 0.68421054, 0.73099416, 0.69005847,\n",
       "        0.70175439, 0.71345031, 0.67251462, 0.69590646, 0.70175439,\n",
       "        0.71929824, 0.69005847, 0.68421054, 0.67251462, 0.68421054,\n",
       "        0.71345031, 0.70175439, 0.70760232, 0.69005847, 0.70760232,\n",
       "        0.68421054, 0.67251462, 0.69005847, 0.71345031, 0.74269009,\n",
       "        0.69005847, 0.69590646, 0.69005847, 0.70175439, 0.7368421 ,\n",
       "        0.69590646, 0.71929824, 0.69005847, 0.70175439, 0.72514617,\n",
       "        0.70175439, 0.71929824, 0.69590646, 0.73099416, 0.73099416,\n",
       "        0.70760232, 0.71345031, 0.71929824, 0.7368421 , 0.71929824,\n",
       "        0.73099416, 0.69005847, 0.67251462, 0.69590646, 0.71345031,\n",
       "        0.71345031, 0.72514617, 0.73099416, 0.69005847, 0.67251462,\n",
       "        0.69590646, 0.68421054, 0.73099416, 0.69005847, 0.74269009,\n",
       "        0.69005847, 0.68421054, 0.69590646, 0.69590646, 0.7368421 ,\n",
       "        0.71929824, 0.71929824, 0.69005847, 0.68421054, 0.70760232,\n",
       "        0.71345031, 0.74853802, 0.73099416, 0.73099416, 0.69005847,\n",
       "        0.69005847, 0.70175439, 0.70175439, 0.71345031, 0.69590646,\n",
       "        0.7368421 , 0.69005847, 0.67836255, 0.69005847, 0.7368421 ,\n",
       "        0.72514617, 0.73099416, 0.75438595, 0.69005847, 0.70175439,\n",
       "        0.71929824, 0.69590646, 0.7368421 , 0.70175439, 0.71345031,\n",
       "        0.69005847, 0.70760232, 0.70175439, 0.70760232, 0.72514617,\n",
       "        0.69590646, 0.7368421 , 0.69005847, 0.73099416, 0.72514617,\n",
       "        0.71929824, 0.73099416, 0.6608187 , 0.7368421 , 0.69005847,\n",
       "        0.70175439, 0.67836255, 0.67836255, 0.72514617, 0.53801167,\n",
       "        0.74269009, 0.32748538, 0.64912283, 0.71345031, 0.71929824,\n",
       "        0.74269009, 0.69005847, 0.74853802, 0.67836255, 0.69590646,\n",
       "        0.71929824, 0.72514617, 0.72514617, 0.71345031, 0.7368421 ,\n",
       "        0.69005847, 0.70175439, 0.74853802, 0.6608187 , 0.76023394,\n",
       "        0.71929824, 0.7368421 , 0.69005847, 0.70760232, 0.72514617,\n",
       "        0.40935671, 0.7368421 , 0.63742691, 0.7368421 , 0.69005847,\n",
       "        0.71345031, 0.71929824, 0.70760232, 0.7368421 , 0.69590646,\n",
       "        0.71929824, 0.69005847, 0.69590646, 0.71929824, 0.70175439,\n",
       "        0.74269009, 0.72514617, 0.73099416, 0.69005847, 0.71345031,\n",
       "        0.69590646, 0.67251462, 0.76023394, 0.70760232, 0.7368421 ,\n",
       "        0.69005847, 0.71345031, 0.71345031, 0.71345031, 0.74269009,\n",
       "        0.63157892, 0.74269009, 0.68421054, 0.69005847, 0.70175439,\n",
       "        0.70175439, 0.7368421 , 0.70760232, 0.74853802, 0.69005847,\n",
       "        0.69590646, 0.69005847, 0.72514617, 0.72514617, 0.72514617,\n",
       "        0.71345031, 0.69005847, 0.70760232, 0.68421054, 0.70760232,\n",
       "        0.7368421 , 0.71929824, 0.74269009, 0.69005847, 0.68421054,\n",
       "        0.70760232, 0.69590646, 0.69005847, 0.69590646, 0.74269009,\n",
       "        0.30994153, 0.71929824, 0.7368421 , 0.5847953 , 0.69005847,\n",
       "        0.74853802, 0.74269009, 0.63742691, 0.71929824, 0.71929824,\n",
       "        0.69590646, 0.69590646, 0.72514617, 0.73099416, 0.69590646,\n",
       "        0.71345031, 0.7368421 , 0.71345031, 0.70760232, 0.59064329,\n",
       "        0.74269009, 0.69005847, 0.73099416, 0.72514617, 0.70175439,\n",
       "        0.69590646, 0.71345031, 0.74269009, 0.69005847, 0.69005847,\n",
       "        0.71929824, 0.70760232, 0.71345031, 0.71929824, 0.71345031,\n",
       "        0.69005847, 0.67836255, 0.7368421 , 0.73099416, 0.7368421 ,\n",
       "        0.70760232, 0.71345031, 0.69005847, 0.69590646, 0.69590646,\n",
       "        0.71345031, 0.74269009, 0.7368421 , 0.75438595, 0.69590646,\n",
       "        0.71345031, 0.71929824, 0.72514617, 0.72514617, 0.74269009,\n",
       "        0.72514617, 0.69005847, 0.72514617, 0.71929824, 0.57894737,\n",
       "        0.76023394, 0.7368421 , 0.72514617, 0.69005847, 0.68421054,\n",
       "        0.71929824, 0.71345031, 0.7368421 , 0.66666669, 0.7368421 ,\n",
       "        0.69005847, 0.69590646, 0.68421054, 0.70760232, 0.73099416,\n",
       "        0.67836255, 0.70760232, 0.69005847, 0.70175439, 0.70175439,\n",
       "        0.69005847]),\n",
       " 'mean_test_score': array([0.73099416, 0.71345031, 0.74463938, 0.66081872, 0.70955165,\n",
       "        0.72904483, 0.7037037 , 0.68810918, 0.72319688, 0.72514619,\n",
       "        0.66081872, 0.70370368, 0.71734893, 0.63352825, 0.71929822,\n",
       "        0.70955167, 0.71734891, 0.66081872, 0.68615985, 0.71734893,\n",
       "        0.70175439, 0.7153996 , 0.7037037 , 0.7270955 , 0.66081872,\n",
       "        0.70175437, 0.71345029, 0.69590642, 0.72319688, 0.68810918,\n",
       "        0.73489279, 0.66081872, 0.6920078 , 0.71929824, 0.69980506,\n",
       "        0.7153996 , 0.70955165, 0.73294348, 0.66081872, 0.70565301,\n",
       "        0.71929824, 0.69980506, 0.70955165, 0.6920078 , 0.7368421 ,\n",
       "        0.66081872, 0.69785575, 0.71734893, 0.69005849, 0.72514619,\n",
       "        0.71734893, 0.72514619, 0.66081872, 0.69785575, 0.70760234,\n",
       "        0.69200782, 0.70955167, 0.69785575, 0.74074074, 0.66081872,\n",
       "        0.68810916, 0.72514619, 0.68615985, 0.69980508, 0.69785575,\n",
       "        0.72514619, 0.66081872, 0.7153996 , 0.6920078 , 0.70565303,\n",
       "        0.71345029, 0.70370372, 0.72904483, 0.66081872, 0.70175437,\n",
       "        0.69785577, 0.66471734, 0.67446393, 0.69395713, 0.73099416,\n",
       "        0.66081872, 0.72124757, 0.69980508, 0.69980508, 0.71150096,\n",
       "        0.69590644, 0.7270955 , 0.65692008, 0.68810916, 0.73099415,\n",
       "        0.69980506, 0.72514619, 0.67446395, 0.72124757, 0.5925926 ,\n",
       "        0.71734891, 0.72709552, 0.70565303, 0.72709552, 0.68421054,\n",
       "        0.73099416, 0.65497077, 0.70760234, 0.71345029, 0.6920078 ,\n",
       "        0.71734893, 0.68421052, 0.74269007, 0.66081872, 0.7037037 ,\n",
       "        0.7231969 , 0.71345029, 0.72709552, 0.70175439, 0.72709552,\n",
       "        0.66081872, 0.71345029, 0.70565303, 0.70565303, 0.72904483,\n",
       "        0.70175437, 0.73099415, 0.66081872, 0.70175439, 0.71929824,\n",
       "        0.71539962, 0.73879143, 0.72319688, 0.72514621, 0.66081872,\n",
       "        0.7037037 , 0.70955165, 0.70175439, 0.71150098, 0.69980508,\n",
       "        0.73099416, 0.66081872, 0.70565301, 0.69005849, 0.73294346,\n",
       "        0.73294346, 0.71345029, 0.73489277, 0.66081872, 0.69005847,\n",
       "        0.70175439, 0.70370372, 0.71929824, 0.7037037 , 0.74074076,\n",
       "        0.66081872, 0.70760234, 0.71929824, 0.72124755, 0.7153996 ,\n",
       "        0.71345031, 0.72904483, 0.66081872, 0.72124757, 0.72319688,\n",
       "        0.72514621, 0.71929826, 0.70565301, 0.73489279, 0.66081872,\n",
       "        0.7153996 , 0.69785573, 0.67641324, 0.68810916, 0.65107212,\n",
       "        0.73294348, 0.46978558, 0.70760236, 0.71734893, 0.72319688,\n",
       "        0.6920078 , 0.68810916, 0.73099415, 0.65107212, 0.69785577,\n",
       "        0.72319688, 0.72904483, 0.68615985, 0.71734893, 0.72709552,\n",
       "        0.66081872, 0.72319688, 0.72904483, 0.64717348, 0.72319688,\n",
       "        0.71734893, 0.72904483, 0.66081872, 0.71345029, 0.72904483,\n",
       "        0.6276803 , 0.69590644, 0.65886941, 0.72514619, 0.65497077,\n",
       "        0.71929824, 0.7368421 , 0.69785573, 0.71150096, 0.70955167,\n",
       "        0.71929824, 0.66081872, 0.70565303, 0.72319688, 0.7037037 ,\n",
       "        0.71734893, 0.70565301, 0.73684212, 0.66276803, 0.70760234,\n",
       "        0.70565303, 0.69785575, 0.73099415, 0.70565301, 0.74074074,\n",
       "        0.66081872, 0.72514621, 0.71734893, 0.72124755, 0.70760234,\n",
       "        0.6920078 , 0.73099415, 0.65886941, 0.70760234, 0.73294348,\n",
       "        0.6920078 , 0.7270955 , 0.7153996 , 0.73879143, 0.66081872,\n",
       "        0.70955165, 0.70760234, 0.64717348, 0.72514619, 0.68031188,\n",
       "        0.72319688, 0.66081872, 0.70955165, 0.70565303, 0.71929824,\n",
       "        0.73294348, 0.71734891, 0.73684212, 0.66081872, 0.71345029,\n",
       "        0.72514619, 0.71734895, 0.66276803, 0.71929826, 0.73294348,\n",
       "        0.53996102, 0.72904483, 0.73879143, 0.65497077, 0.66666667,\n",
       "        0.7037037 , 0.73294348, 0.53216375, 0.70955167, 0.72514619,\n",
       "        0.71345029, 0.66276805, 0.72124755, 0.73489279, 0.65107212,\n",
       "        0.71539962, 0.7368421 , 0.72904483, 0.66861598, 0.69590644,\n",
       "        0.74074076, 0.66081872, 0.72319688, 0.72319686, 0.68421052,\n",
       "        0.67251464, 0.70955165, 0.73294348, 0.4834308 , 0.7153996 ,\n",
       "        0.73099416, 0.71345029, 0.67446395, 0.70955165, 0.72319688,\n",
       "        0.59259259, 0.7037037 , 0.7368421 , 0.73294348, 0.69590644,\n",
       "        0.70955165, 0.73489281, 0.60233919, 0.70955167, 0.71539962,\n",
       "        0.70175439, 0.69395713, 0.72904483, 0.72904483, 0.65302145,\n",
       "        0.71734893, 0.71734893, 0.68810916, 0.69785575, 0.71929826,\n",
       "        0.72514617, 0.63352827, 0.71929824, 0.72904483, 0.668616  ,\n",
       "        0.72904485, 0.70955165, 0.72709552, 0.66081872, 0.69980508,\n",
       "        0.71539962, 0.70175439, 0.70760232, 0.69005849, 0.7368421 ,\n",
       "        0.66276803, 0.72319688, 0.70760234, 0.71734891, 0.70760234,\n",
       "        0.70175439, 0.73294348, 0.66081872, 0.70760234, 0.71734893,\n",
       "        0.70760232]),\n",
       " 'std_test_score': array([0.03443181, 0.02387418, 0.00729367, 0.02658515, 0.01201638,\n",
       "        0.00275677, 0.0306979 , 0.05105576, 0.0099396 , 0.00477484,\n",
       "        0.02658515, 0.01929726, 0.01807724, 0.04538186, 0.00827025,\n",
       "        0.03179241, 0.00729368, 0.02658515, 0.04436572, 0.00551348,\n",
       "        0.02658514, 0.00729367, 0.04060948, 0.00729368, 0.02658515,\n",
       "        0.05506606, 0.00954966, 0.02658515, 0.01929726, 0.02917471,\n",
       "        0.00993962, 0.02658515, 0.00275677, 0.01721589, 0.01807722,\n",
       "        0.02450255, 0.03250162, 0.00275674, 0.02658515, 0.01676865,\n",
       "        0.01263302, 0.01676868, 0.01201638, 0.02153091, 0.02526603,\n",
       "        0.02658515, 0.0099396 , 0.00993962, 0.01721592, 0.01263303,\n",
       "        0.02256498, 0.00477484, 0.02658515, 0.01102702, 0.01721592,\n",
       "        0.01929726, 0.0099396 , 0.0099396 , 0.0099396 , 0.02658515,\n",
       "        0.02355369, 0.00954966, 0.01929727, 0.01458735, 0.00551351,\n",
       "        0.00477484, 0.02658515, 0.02256497, 0.01987922, 0.02256497,\n",
       "        0.00477484, 0.00729368, 0.01676868, 0.02658515, 0.01721592,\n",
       "        0.0120164 , 0.02450255, 0.03519584, 0.01378376, 0.01263302,\n",
       "        0.02658515, 0.01807722, 0.00993962, 0.00275674, 0.02450255,\n",
       "        0.00954966, 0.00729368, 0.02629773, 0.02355369, 0.00477484,\n",
       "        0.01676868, 0.00827025, 0.01929726, 0.022565  , 0.12868451,\n",
       "        0.00729368, 0.01534894, 0.0099396 , 0.0099396 , 0.04554901,\n",
       "        0.        , 0.02658514, 0.02658515, 0.012633  , 0.03454201,\n",
       "        0.00551348, 0.02904417, 0.00827025, 0.02658515, 0.02205401,\n",
       "        0.01987922, 0.02981882, 0.0099396 , 0.00954968, 0.01201641,\n",
       "        0.02658515, 0.02081301, 0.00729367, 0.0099396 , 0.00729368,\n",
       "        0.01721592, 0.02526605, 0.02658515, 0.01263302, 0.00954968,\n",
       "        0.0120164 , 0.00993963, 0.00729367, 0.00827025, 0.02658515,\n",
       "        0.0120164 , 0.01102699, 0.        , 0.00275677, 0.00275674,\n",
       "        0.01263302, 0.02658515, 0.02403281, 0.01909934, 0.00551351,\n",
       "        0.00551351, 0.03789908, 0.01378376, 0.02658515, 0.02081302,\n",
       "        0.01263302, 0.00729368, 0.01263302, 0.01676865, 0.02153091,\n",
       "        0.02658515, 0.00954966, 0.0143245 , 0.00993962, 0.01807723,\n",
       "        0.02481075, 0.01102699, 0.02658515, 0.00729368, 0.00729367,\n",
       "        0.01263303, 0.00827025, 0.03250163, 0.00275674, 0.02658515,\n",
       "        0.0099396 , 0.01378376, 0.0120164 , 0.03387553, 0.08492408,\n",
       "        0.01807724, 0.13709124, 0.04162603, 0.00551348, 0.0099396 ,\n",
       "        0.04588151, 0.00275674, 0.01721592, 0.02153091, 0.0120164 ,\n",
       "        0.00551351, 0.00993963, 0.03646838, 0.01458735, 0.02256497,\n",
       "        0.02658515, 0.01676867, 0.01458736, 0.06517837, 0.03387555,\n",
       "        0.02153093, 0.01102699, 0.02658515, 0.00477484, 0.00275677,\n",
       "        0.15504124, 0.03604919, 0.06158118, 0.00827025, 0.02658514,\n",
       "        0.00477482, 0.0143245 , 0.01378376, 0.02797797, 0.01534894,\n",
       "        0.00477482, 0.02658515, 0.00729367, 0.0099396 , 0.00275674,\n",
       "        0.02153093, 0.01676865, 0.00477484, 0.02403281, 0.02658515,\n",
       "        0.02256497, 0.02153091, 0.02188105, 0.00275674, 0.01458738,\n",
       "        0.02658515, 0.01263303, 0.00993962, 0.00551348, 0.03443183,\n",
       "        0.04332577, 0.00827028, 0.02450255, 0.01263303, 0.02256498,\n",
       "        0.02715085, 0.01378376, 0.00729367, 0.00729368, 0.02658515,\n",
       "        0.01201638, 0.02481078, 0.060082  , 0.00477484, 0.04782785,\n",
       "        0.00729367, 0.02658515, 0.01676868, 0.01929726, 0.00954968,\n",
       "        0.00275674, 0.00729368, 0.00477484, 0.02658515, 0.02081301,\n",
       "        0.01263303, 0.01929726, 0.02715085, 0.04162603, 0.0072937 ,\n",
       "        0.16439118, 0.00993963, 0.00275677, 0.04985075, 0.02188103,\n",
       "        0.03583778, 0.00993962, 0.14067539, 0.0099396 , 0.01721592,\n",
       "        0.0172159 , 0.02878134, 0.01458735, 0.00993962, 0.04116709,\n",
       "        0.00275674, 0.00954968, 0.01807722, 0.03353732, 0.07458528,\n",
       "        0.00275677, 0.02658515, 0.00551351, 0.00275674, 0.01721592,\n",
       "        0.02526605, 0.01458736, 0.0072937 , 0.15101866, 0.01929726,\n",
       "        0.02081302, 0.00477484, 0.03975843, 0.00729368, 0.0099396 ,\n",
       "        0.11776847, 0.01807724, 0.        , 0.0072937 , 0.03342383,\n",
       "        0.02629776, 0.01534894, 0.10793659, 0.01534894, 0.01458735,\n",
       "        0.01263302, 0.0406095 , 0.01102699, 0.02153091, 0.04171724,\n",
       "        0.00275674, 0.00275674, 0.03708829, 0.02153091, 0.01721592,\n",
       "        0.        , 0.0641204 , 0.00477482, 0.01807722, 0.0634053 ,\n",
       "        0.02629773, 0.03069787, 0.01201641, 0.02658515, 0.0120164 ,\n",
       "        0.01458735, 0.01263302, 0.03342384, 0.02188103, 0.00954968,\n",
       "        0.02403281, 0.01929724, 0.01909934, 0.01378376, 0.02526605,\n",
       "        0.0290442 , 0.01807724, 0.02658515, 0.02188103, 0.0120164 ,\n",
       "        0.0143245 ]),\n",
       " 'rank_test_score': array([ 33, 143,   1, 284, 163,  44, 200, 257,  81,  69, 284, 208, 115,\n",
       "        328, 113, 158, 128, 284, 264, 115, 209, 136, 200,  63, 284, 217,\n",
       "        145, 242,  81, 257,  18, 284, 246, 104, 225, 136, 163,  22, 284,\n",
       "        193, 104, 225, 163, 246,  12, 284, 230, 115, 253,  69, 115,  69,\n",
       "        284, 230, 174, 245, 158, 230,   5, 284, 259,  69, 264, 220, 230,\n",
       "         69, 284, 136, 246, 186, 145, 198,  44, 284, 217, 228, 279, 274,\n",
       "        243,  33, 284,  95, 220, 220, 156, 238,  63, 317, 259,  38, 225,\n",
       "         69, 272,  95, 331, 128,  57, 186,  57, 267,  33, 318, 174, 145,\n",
       "        246, 115, 268,   2, 284, 200,  80, 145,  57, 209,  57, 284, 145,\n",
       "        186, 186,  44, 217,  38, 284, 209, 104, 132,   7,  81,  66, 284,\n",
       "        200, 163, 209, 155, 220,  33, 284, 193, 253,  31,  31, 145,  21,\n",
       "        284, 256, 209, 198, 104, 200,   3, 284, 174, 104,  98, 136, 143,\n",
       "         44, 284,  95,  81,  66, 101, 193,  18, 284, 136, 236, 271, 259,\n",
       "        322,  22, 336, 173, 115,  81, 246, 259,  38, 322, 228,  81,  44,\n",
       "        264, 115,  57, 284,  81,  44, 325,  81, 115,  44, 284, 145,  44,\n",
       "        329, 238, 315,  69, 318, 104,  12, 236, 156, 158, 104, 284, 186,\n",
       "         81, 200, 115, 193,  10, 281, 174, 186, 230,  38, 193,   5, 284,\n",
       "         66, 115,  98, 174, 246,  38, 315, 174,  22, 246,  63, 136,   7,\n",
       "        284, 163, 174, 325,  69, 270,  81, 284, 163, 186, 104,  22, 128,\n",
       "         10, 284, 145,  69, 114, 281, 101,  22, 333,  44,   7, 318, 278,\n",
       "        200,  22, 334, 158,  69, 145, 280,  98,  18, 322, 132,  12,  44,\n",
       "        277, 238,   3, 284,  81,  94, 268, 275, 163,  22, 335, 136,  33,\n",
       "        145, 272, 163,  81, 332, 200,  12,  22, 238, 163,  17, 330, 158,\n",
       "        132, 209, 243,  44,  44, 321, 115, 115, 259, 230, 101,  79, 327,\n",
       "        104,  44, 276,  43, 163,  57, 284, 220, 132, 209, 184, 253,  12,\n",
       "        281,  81, 174, 128, 174, 209,  22, 284, 174, 115, 184])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Training Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "342/342 [==============================] - 1s 1ms/sample - loss: 0.6770 - acc: 0.5760\n",
      "Epoch 2/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.6228 - acc: 0.6754\n",
      "Epoch 3/100\n",
      "342/342 [==============================] - 0s 139us/sample - loss: 0.6106 - acc: 0.6725\n",
      "Epoch 4/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.6288 - acc: 0.6725\n",
      "Epoch 5/100\n",
      "342/342 [==============================] - 0s 166us/sample - loss: 0.6389 - acc: 0.6696\n",
      "Epoch 6/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.6356 - acc: 0.6754\n",
      "Epoch 7/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.6214 - acc: 0.6725\n",
      "Epoch 8/100\n",
      "342/342 [==============================] - 0s 155us/sample - loss: 0.6091 - acc: 0.6667\n",
      "Epoch 9/100\n",
      "342/342 [==============================] - 0s 155us/sample - loss: 0.6129 - acc: 0.6813\n",
      "Epoch 10/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.6175 - acc: 0.6842\n",
      "Epoch 11/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.6097 - acc: 0.6930\n",
      "Epoch 12/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.6211 - acc: 0.6930\n",
      "Epoch 13/100\n",
      "342/342 [==============================] - 0s 397us/sample - loss: 0.5725 - acc: 0.6784\n",
      "Epoch 14/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6086 - acc: 0.6813\n",
      "Epoch 15/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5936 - acc: 0.6813\n",
      "Epoch 16/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5892 - acc: 0.6901\n",
      "Epoch 17/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5723 - acc: 0.6871\n",
      "Epoch 18/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5834 - acc: 0.6813\n",
      "Epoch 19/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5758 - acc: 0.6930\n",
      "Epoch 20/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5909 - acc: 0.6959\n",
      "Epoch 21/100\n",
      "342/342 [==============================] - 0s 353us/sample - loss: 0.5894 - acc: 0.6901\n",
      "Epoch 22/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5743 - acc: 0.6959\n",
      "Epoch 23/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5677 - acc: 0.6988\n",
      "Epoch 24/100\n",
      "342/342 [==============================] - 0s 155us/sample - loss: 0.5809 - acc: 0.6871\n",
      "Epoch 25/100\n",
      "342/342 [==============================] - 0s 493us/sample - loss: 0.5840 - acc: 0.6754\n",
      "Epoch 26/100\n",
      "342/342 [==============================] - 0s 155us/sample - loss: 0.5847 - acc: 0.6959\n",
      "Epoch 27/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5842 - acc: 0.6842\n",
      "Epoch 28/100\n",
      "342/342 [==============================] - 0s 190us/sample - loss: 0.5505 - acc: 0.7076\n",
      "Epoch 29/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5790 - acc: 0.6959\n",
      "Epoch 30/100\n",
      "342/342 [==============================] - 0s 152us/sample - loss: 0.5668 - acc: 0.6959\n",
      "Epoch 31/100\n",
      "342/342 [==============================] - 0s 321us/sample - loss: 0.5549 - acc: 0.7135\n",
      "Epoch 32/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5646 - acc: 0.7047\n",
      "Epoch 33/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5684 - acc: 0.7076\n",
      "Epoch 34/100\n",
      "342/342 [==============================] - 0s 143us/sample - loss: 0.5726 - acc: 0.6988\n",
      "Epoch 35/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5537 - acc: 0.7251\n",
      "Epoch 36/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5527 - acc: 0.7310\n",
      "Epoch 37/100\n",
      "342/342 [==============================] - 0s 152us/sample - loss: 0.5602 - acc: 0.6813\n",
      "Epoch 38/100\n",
      "342/342 [==============================] - 0s 414us/sample - loss: 0.5831 - acc: 0.7164\n",
      "Epoch 39/100\n",
      "342/342 [==============================] - 0s 166us/sample - loss: 0.5394 - acc: 0.7164\n",
      "Epoch 40/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5495 - acc: 0.7251\n",
      "Epoch 41/100\n",
      "342/342 [==============================] - 0s 192us/sample - loss: 0.5455 - acc: 0.7164\n",
      "Epoch 42/100\n",
      "342/342 [==============================] - 0s 146us/sample - loss: 0.5458 - acc: 0.7047\n",
      "Epoch 43/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5574 - acc: 0.7368\n",
      "Epoch 44/100\n",
      "342/342 [==============================] - 0s 187us/sample - loss: 0.5527 - acc: 0.7047\n",
      "Epoch 45/100\n",
      "342/342 [==============================] - 0s 172us/sample - loss: 0.5460 - acc: 0.7193\n",
      "Epoch 46/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5512 - acc: 0.7222\n",
      "Epoch 47/100\n",
      "342/342 [==============================] - 0s 160us/sample - loss: 0.5446 - acc: 0.7602\n",
      "Epoch 48/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5464 - acc: 0.7368\n",
      "Epoch 49/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5421 - acc: 0.7368\n",
      "Epoch 50/100\n",
      "342/342 [==============================] - 0s 370us/sample - loss: 0.5454 - acc: 0.7456\n",
      "Epoch 51/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5213 - acc: 0.7339\n",
      "Epoch 52/100\n",
      "342/342 [==============================] - 0s 131us/sample - loss: 0.5242 - acc: 0.7222\n",
      "Epoch 53/100\n",
      "342/342 [==============================] - 0s 181us/sample - loss: 0.5355 - acc: 0.7193\n",
      "Epoch 54/100\n",
      "342/342 [==============================] - 0s 295us/sample - loss: 0.5223 - acc: 0.7485\n",
      "Epoch 55/100\n",
      "342/342 [==============================] - 0s 411us/sample - loss: 0.5200 - acc: 0.7398\n",
      "Epoch 56/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5280 - acc: 0.7485\n",
      "Epoch 57/100\n",
      "342/342 [==============================] - 0s 134us/sample - loss: 0.5134 - acc: 0.7485\n",
      "Epoch 58/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5338 - acc: 0.7398\n",
      "Epoch 59/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5312 - acc: 0.7573\n",
      "Epoch 60/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5430 - acc: 0.7573\n",
      "Epoch 61/100\n",
      "342/342 [==============================] - 0s 143us/sample - loss: 0.5221 - acc: 0.7485\n",
      "Epoch 62/100\n",
      "342/342 [==============================] - 0s 126us/sample - loss: 0.5117 - acc: 0.7632\n",
      "Epoch 63/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5107 - acc: 0.7602\n",
      "Epoch 64/100\n",
      "342/342 [==============================] - 0s 119us/sample - loss: 0.5216 - acc: 0.7544\n",
      "Epoch 65/100\n",
      "342/342 [==============================] - 0s 149us/sample - loss: 0.5291 - acc: 0.7368\n",
      "Epoch 66/100\n",
      "342/342 [==============================] - 0s 160us/sample - loss: 0.5220 - acc: 0.7427\n",
      "Epoch 67/100\n",
      "342/342 [==============================] - 0s 458us/sample - loss: 0.5412 - acc: 0.7602\n",
      "Epoch 68/100\n",
      "342/342 [==============================] - 0s 236us/sample - loss: 0.5070 - acc: 0.7544\n",
      "Epoch 69/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5180 - acc: 0.7485\n",
      "Epoch 70/100\n",
      "342/342 [==============================] - 0s 160us/sample - loss: 0.5160 - acc: 0.7427\n",
      "Epoch 71/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5332 - acc: 0.7339\n",
      "Epoch 72/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5238 - acc: 0.7602\n",
      "Epoch 73/100\n",
      "342/342 [==============================] - 0s 341us/sample - loss: 0.5056 - acc: 0.7251\n",
      "Epoch 74/100\n",
      "342/342 [==============================] - 0s 134us/sample - loss: 0.5173 - acc: 0.7515\n",
      "Epoch 75/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5154 - acc: 0.7573\n",
      "Epoch 76/100\n",
      "342/342 [==============================] - 0s 124us/sample - loss: 0.5124 - acc: 0.7632\n",
      "Epoch 77/100\n",
      "342/342 [==============================] - ETA: 0s - loss: 0.4587 - acc: 0.781 - 0s 114us/sample - loss: 0.5039 - acc: 0.7485\n",
      "Epoch 78/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5070 - acc: 0.7485\n",
      "Epoch 79/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5046 - acc: 0.7690\n",
      "Epoch 80/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5351 - acc: 0.7193\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 0s 160us/sample - loss: 0.5199 - acc: 0.7427\n",
      "Epoch 82/100\n",
      "342/342 [==============================] - 0s 152us/sample - loss: 0.5133 - acc: 0.7251\n",
      "Epoch 83/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.4945 - acc: 0.7632\n",
      "Epoch 84/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5166 - acc: 0.7544\n",
      "Epoch 85/100\n",
      "342/342 [==============================] - 0s 97us/sample - loss: 0.5180 - acc: 0.7544\n",
      "Epoch 86/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5020 - acc: 0.7661\n",
      "Epoch 87/100\n",
      "342/342 [==============================] - 0s 297us/sample - loss: 0.4866 - acc: 0.7573\n",
      "Epoch 88/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.4876 - acc: 0.7719\n",
      "Epoch 89/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5192 - acc: 0.7661\n",
      "Epoch 90/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.4998 - acc: 0.7339\n",
      "Epoch 91/100\n",
      "342/342 [==============================] - 0s 93us/sample - loss: 0.5000 - acc: 0.7602\n",
      "Epoch 92/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.4937 - acc: 0.7515\n",
      "Epoch 93/100\n",
      "342/342 [==============================] - 0s 113us/sample - loss: 0.4862 - acc: 0.7953\n",
      "Epoch 94/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.4969 - acc: 0.7719\n",
      "Epoch 95/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5044 - acc: 0.7602\n",
      "Epoch 96/100\n",
      "342/342 [==============================] - 0s 292us/sample - loss: 0.4784 - acc: 0.7778\n",
      "Epoch 97/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5274 - acc: 0.7339\n",
      "Epoch 98/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.4957 - acc: 0.7544\n",
      "Epoch 99/100\n",
      "342/342 [==============================] - 0s 172us/sample - loss: 0.4996 - acc: 0.7807\n",
      "Epoch 100/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5044 - acc: 0.7865\n",
      "171/171 [==============================] - 0s 939us/sample - loss: 0.5424 - acc: 0.7485\n",
      "Epoch 1/100\n",
      "342/342 [==============================] - 1s 2ms/sample - loss: 0.6734 - acc: 0.5848\n",
      "Epoch 2/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.6452 - acc: 0.6462\n",
      "Epoch 3/100\n",
      "342/342 [==============================] - 0s 338us/sample - loss: 0.6416 - acc: 0.6374\n",
      "Epoch 4/100\n",
      "342/342 [==============================] - 0s 116us/sample - loss: 0.6206 - acc: 0.6696\n",
      "Epoch 5/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6366 - acc: 0.6579\n",
      "Epoch 6/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6312 - acc: 0.6550\n",
      "Epoch 7/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.6394 - acc: 0.6491\n",
      "Epoch 8/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.6197 - acc: 0.6491\n",
      "Epoch 9/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.6167 - acc: 0.6667\n",
      "Epoch 10/100\n",
      "342/342 [==============================] - 0s 140us/sample - loss: 0.6219 - acc: 0.6637\n",
      "Epoch 11/100\n",
      "342/342 [==============================] - 0s 96us/sample - loss: 0.6105 - acc: 0.6725\n",
      "Epoch 12/100\n",
      "342/342 [==============================] - 0s 324us/sample - loss: 0.6215 - acc: 0.6608\n",
      "Epoch 13/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.6034 - acc: 0.6667\n",
      "Epoch 14/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6097 - acc: 0.6667\n",
      "Epoch 15/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5954 - acc: 0.6637\n",
      "Epoch 16/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.6102 - acc: 0.6637\n",
      "Epoch 17/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.6044 - acc: 0.6813\n",
      "Epoch 18/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5959 - acc: 0.6813\n",
      "Epoch 19/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5917 - acc: 0.6813\n",
      "Epoch 20/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5990 - acc: 0.6959\n",
      "Epoch 21/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5888 - acc: 0.6813\n",
      "Epoch 22/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5805 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5916 - acc: 0.6784\n",
      "Epoch 24/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5889 - acc: 0.6725\n",
      "Epoch 25/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5653 - acc: 0.7164\n",
      "Epoch 26/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5960 - acc: 0.6871\n",
      "Epoch 27/100\n",
      "342/342 [==============================] - 0s 309us/sample - loss: 0.5782 - acc: 0.6842\n",
      "Epoch 28/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5842 - acc: 0.7018\n",
      "Epoch 29/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5620 - acc: 0.7398\n",
      "Epoch 30/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5745 - acc: 0.6959\n",
      "Epoch 31/100\n",
      "342/342 [==============================] - 0s 123us/sample - loss: 0.5703 - acc: 0.6988\n",
      "Epoch 32/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5709 - acc: 0.6842\n",
      "Epoch 33/100\n",
      "342/342 [==============================] - 0s 110us/sample - loss: 0.5705 - acc: 0.7135\n",
      "Epoch 34/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5532 - acc: 0.7135\n",
      "Epoch 35/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5579 - acc: 0.7193\n",
      "Epoch 36/100\n",
      "342/342 [==============================] - 0s 309us/sample - loss: 0.5683 - acc: 0.7018\n",
      "Epoch 37/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5660 - acc: 0.7047\n",
      "Epoch 38/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5490 - acc: 0.7135\n",
      "Epoch 39/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5590 - acc: 0.7427\n",
      "Epoch 40/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5536 - acc: 0.7193\n",
      "Epoch 41/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5578 - acc: 0.7105\n",
      "Epoch 42/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5348 - acc: 0.7251\n",
      "Epoch 43/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5588 - acc: 0.7018\n",
      "Epoch 44/100\n",
      "342/342 [==============================] - 0s 163us/sample - loss: 0.5363 - acc: 0.7339\n",
      "Epoch 45/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5497 - acc: 0.6930\n",
      "Epoch 46/100\n",
      "342/342 [==============================] - 0s 131us/sample - loss: 0.5414 - acc: 0.7427\n",
      "Epoch 47/100\n",
      "342/342 [==============================] - 0s 140us/sample - loss: 0.5567 - acc: 0.7281\n",
      "Epoch 48/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5438 - acc: 0.7485\n",
      "Epoch 49/100\n",
      "342/342 [==============================] - 0s 172us/sample - loss: 0.5416 - acc: 0.7281\n",
      "Epoch 50/100\n",
      "342/342 [==============================] - 0s 306us/sample - loss: 0.5262 - acc: 0.7398\n",
      "Epoch 51/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5443 - acc: 0.7456\n",
      "Epoch 52/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5399 - acc: 0.7398\n",
      "Epoch 53/100\n",
      "342/342 [==============================] - 0s 127us/sample - loss: 0.5419 - acc: 0.7222\n",
      "Epoch 54/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5279 - acc: 0.7515\n",
      "Epoch 55/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5315 - acc: 0.7456\n",
      "Epoch 56/100\n",
      "342/342 [==============================] - 0s 131us/sample - loss: 0.5336 - acc: 0.7398\n",
      "Epoch 57/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5187 - acc: 0.7427\n",
      "Epoch 58/100\n",
      "342/342 [==============================] - 0s 332us/sample - loss: 0.5327 - acc: 0.7544\n",
      "Epoch 59/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5260 - acc: 0.7339\n",
      "Epoch 60/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5183 - acc: 0.7368\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5189 - acc: 0.7456\n",
      "Epoch 62/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5264 - acc: 0.7602\n",
      "Epoch 63/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5057 - acc: 0.7368\n",
      "Epoch 64/100\n",
      "342/342 [==============================] - 0s 104us/sample - loss: 0.5369 - acc: 0.7339\n",
      "Epoch 65/100\n",
      "342/342 [==============================] - 0s 104us/sample - loss: 0.5197 - acc: 0.7661\n",
      "Epoch 66/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5118 - acc: 0.7281\n",
      "Epoch 67/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5477 - acc: 0.7456\n",
      "Epoch 68/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5274 - acc: 0.7485\n",
      "Epoch 69/100\n",
      "342/342 [==============================] - 0s 103us/sample - loss: 0.5407 - acc: 0.7281\n",
      "Epoch 70/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5092 - acc: 0.7749\n",
      "Epoch 71/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5319 - acc: 0.7310\n",
      "Epoch 72/100\n",
      "342/342 [==============================] - 0s 332us/sample - loss: 0.5073 - acc: 0.7398\n",
      "Epoch 73/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5131 - acc: 0.7544\n",
      "Epoch 74/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5115 - acc: 0.7310\n",
      "Epoch 75/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5448 - acc: 0.7135\n",
      "Epoch 76/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5238 - acc: 0.7193\n",
      "Epoch 77/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5170 - acc: 0.7310\n",
      "Epoch 78/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5120 - acc: 0.7515\n",
      "Epoch 79/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5210 - acc: 0.7632\n",
      "Epoch 80/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5165 - acc: 0.7427\n",
      "Epoch 81/100\n",
      "342/342 [==============================] - 0s 166us/sample - loss: 0.5083 - acc: 0.7690\n",
      "Epoch 82/100\n",
      "342/342 [==============================] - 0s 231us/sample - loss: 0.5029 - acc: 0.7661\n",
      "Epoch 83/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5035 - acc: 0.7515\n",
      "Epoch 84/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.4847 - acc: 0.7602\n",
      "Epoch 85/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5076 - acc: 0.7573\n",
      "Epoch 86/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5061 - acc: 0.7544\n",
      "Epoch 87/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.4792 - acc: 0.7719\n",
      "Epoch 88/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5097 - acc: 0.7398\n",
      "Epoch 89/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5009 - acc: 0.7485\n",
      "Epoch 90/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.4791 - acc: 0.7778\n",
      "Epoch 91/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5045 - acc: 0.7602\n",
      "Epoch 92/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.4991 - acc: 0.7602\n",
      "Epoch 93/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.4860 - acc: 0.7749\n",
      "Epoch 94/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.4992 - acc: 0.7719\n",
      "Epoch 95/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.4906 - acc: 0.7602\n",
      "Epoch 96/100\n",
      "342/342 [==============================] - 0s 268us/sample - loss: 0.5132 - acc: 0.7310\n",
      "Epoch 97/100\n",
      "342/342 [==============================] - 0s 146us/sample - loss: 0.5054 - acc: 0.7544\n",
      "Epoch 98/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.4854 - acc: 0.7807\n",
      "Epoch 99/100\n",
      "342/342 [==============================] - 0s 119us/sample - loss: 0.4974 - acc: 0.7690\n",
      "Epoch 100/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.4960 - acc: 0.7632\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.5995 - acc: 0.6901\n",
      "Epoch 1/100\n",
      "342/342 [==============================] - 0s 1ms/sample - loss: 0.6566 - acc: 0.6082\n",
      "Epoch 2/100\n",
      "342/342 [==============================] - 0s 344us/sample - loss: 0.6604 - acc: 0.6433\n",
      "Epoch 3/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.6543 - acc: 0.6374\n",
      "Epoch 4/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.6455 - acc: 0.6491\n",
      "Epoch 5/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.6384 - acc: 0.6257\n",
      "Epoch 6/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.6468 - acc: 0.6316\n",
      "Epoch 7/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.6313 - acc: 0.6345\n",
      "Epoch 8/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.6227 - acc: 0.6433\n",
      "Epoch 9/100\n",
      "342/342 [==============================] - 0s 131us/sample - loss: 0.6293 - acc: 0.6433\n",
      "Epoch 10/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.6227 - acc: 0.6462\n",
      "Epoch 11/100\n",
      "342/342 [==============================] - 0s 321us/sample - loss: 0.6204 - acc: 0.6579\n",
      "Epoch 12/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6233 - acc: 0.6257\n",
      "Epoch 13/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.6353 - acc: 0.6491\n",
      "Epoch 14/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.6173 - acc: 0.6433\n",
      "Epoch 15/100\n",
      "342/342 [==============================] - 0s 155us/sample - loss: 0.6148 - acc: 0.6462\n",
      "Epoch 16/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.6198 - acc: 0.6287\n",
      "Epoch 17/100\n",
      "342/342 [==============================] - 0s 110us/sample - loss: 0.6072 - acc: 0.6696\n",
      "Epoch 18/100\n",
      "342/342 [==============================] - 0s 127us/sample - loss: 0.6122 - acc: 0.6404\n",
      "Epoch 19/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.6186 - acc: 0.6579\n",
      "Epoch 20/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.6188 - acc: 0.6520\n",
      "Epoch 21/100\n",
      "342/342 [==============================] - 0s 127us/sample - loss: 0.6153 - acc: 0.6637\n",
      "Epoch 22/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.6300 - acc: 0.6404\n",
      "Epoch 23/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.6098 - acc: 0.6959\n",
      "Epoch 24/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5931 - acc: 0.6813\n",
      "Epoch 25/100\n",
      "342/342 [==============================] - 0s 341us/sample - loss: 0.6081 - acc: 0.6696\n",
      "Epoch 26/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.6114 - acc: 0.6988\n",
      "Epoch 27/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5978 - acc: 0.6813\n",
      "Epoch 28/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5996 - acc: 0.6813\n",
      "Epoch 29/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5938 - acc: 0.6842\n",
      "Epoch 30/100\n",
      "342/342 [==============================] - 0s 121us/sample - loss: 0.5918 - acc: 0.6696\n",
      "Epoch 31/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5883 - acc: 0.6871\n",
      "Epoch 32/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5956 - acc: 0.6959\n",
      "Epoch 33/100\n",
      "342/342 [==============================] - 0s 359us/sample - loss: 0.5786 - acc: 0.6988\n",
      "Epoch 34/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5820 - acc: 0.6901\n",
      "Epoch 35/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5999 - acc: 0.6930\n",
      "Epoch 36/100\n",
      "342/342 [==============================] - 0s 222us/sample - loss: 0.5927 - acc: 0.6901\n",
      "Epoch 37/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5805 - acc: 0.6988\n",
      "Epoch 38/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5900 - acc: 0.6959\n",
      "Epoch 39/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5875 - acc: 0.6871\n",
      "Epoch 40/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5812 - acc: 0.7281\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 0s 116us/sample - loss: 0.5796 - acc: 0.7105\n",
      "Epoch 42/100\n",
      "342/342 [==============================] - 0s 113us/sample - loss: 0.5687 - acc: 0.7368\n",
      "Epoch 43/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5637 - acc: 0.6988\n",
      "Epoch 44/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5642 - acc: 0.7193\n",
      "Epoch 45/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5777 - acc: 0.7047\n",
      "Epoch 46/100\n",
      "342/342 [==============================] - 0s 315us/sample - loss: 0.5557 - acc: 0.7398\n",
      "Epoch 47/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5823 - acc: 0.6988\n",
      "Epoch 48/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5816 - acc: 0.6901\n",
      "Epoch 49/100\n",
      "342/342 [==============================] - 0s 119us/sample - loss: 0.5707 - acc: 0.6871\n",
      "Epoch 50/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5876 - acc: 0.7222\n",
      "Epoch 51/100\n",
      "342/342 [==============================] - 0s 102us/sample - loss: 0.5646 - acc: 0.7251\n",
      "Epoch 52/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5809 - acc: 0.7018\n",
      "Epoch 53/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5577 - acc: 0.7398\n",
      "Epoch 54/100\n",
      "342/342 [==============================] - 0s 113us/sample - loss: 0.5778 - acc: 0.7456\n",
      "Epoch 55/100\n",
      "342/342 [==============================] - 0s 95us/sample - loss: 0.5568 - acc: 0.7456\n",
      "Epoch 56/100\n",
      "342/342 [==============================] - 0s 268us/sample - loss: 0.5572 - acc: 0.7281\n",
      "Epoch 57/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5566 - acc: 0.7193\n",
      "Epoch 58/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5599 - acc: 0.7251\n",
      "Epoch 59/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5645 - acc: 0.7135\n",
      "Epoch 60/100\n",
      "342/342 [==============================] - 0s 101us/sample - loss: 0.5598 - acc: 0.7310\n",
      "Epoch 61/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5396 - acc: 0.7632\n",
      "Epoch 62/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5494 - acc: 0.7281\n",
      "Epoch 63/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5343 - acc: 0.7398\n",
      "Epoch 64/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5311 - acc: 0.7310\n",
      "Epoch 65/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5434 - acc: 0.7427\n",
      "Epoch 66/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5634 - acc: 0.7251\n",
      "Epoch 67/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5329 - acc: 0.7632\n",
      "Epoch 68/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5724 - acc: 0.7398\n",
      "Epoch 69/100\n",
      "342/342 [==============================] - 0s 113us/sample - loss: 0.5679 - acc: 0.7222\n",
      "Epoch 70/100\n",
      "342/342 [==============================] - 0s 309us/sample - loss: 0.5543 - acc: 0.7281\n",
      "Epoch 71/100\n",
      "342/342 [==============================] - 0s 125us/sample - loss: 0.5358 - acc: 0.7456\n",
      "Epoch 72/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5533 - acc: 0.6959\n",
      "Epoch 73/100\n",
      "342/342 [==============================] - 0s 96us/sample - loss: 0.5411 - acc: 0.7456\n",
      "Epoch 74/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5498 - acc: 0.7251\n",
      "Epoch 75/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5515 - acc: 0.7485\n",
      "Epoch 76/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5464 - acc: 0.7310\n",
      "Epoch 77/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5314 - acc: 0.7544\n",
      "Epoch 78/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5522 - acc: 0.7164\n",
      "Epoch 79/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5304 - acc: 0.7749\n",
      "Epoch 80/100\n",
      "342/342 [==============================] - 0s 324us/sample - loss: 0.5324 - acc: 0.7368\n",
      "Epoch 81/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5305 - acc: 0.7602\n",
      "Epoch 82/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5479 - acc: 0.7135\n",
      "Epoch 83/100\n",
      "342/342 [==============================] - 0s 119us/sample - loss: 0.5350 - acc: 0.7398\n",
      "Epoch 84/100\n",
      "342/342 [==============================] - 0s 116us/sample - loss: 0.5576 - acc: 0.7544\n",
      "Epoch 85/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5425 - acc: 0.7398\n",
      "Epoch 86/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5180 - acc: 0.7456\n",
      "Epoch 87/100\n",
      "342/342 [==============================] - 0s 105us/sample - loss: 0.5230 - acc: 0.7485\n",
      "Epoch 88/100\n",
      "342/342 [==============================] - 0s 128us/sample - loss: 0.5453 - acc: 0.7544\n",
      "Epoch 89/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5572 - acc: 0.7164\n",
      "Epoch 90/100\n",
      "342/342 [==============================] - 0s 117us/sample - loss: 0.5261 - acc: 0.7632\n",
      "Epoch 91/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5325 - acc: 0.7339\n",
      "Epoch 92/100\n",
      "342/342 [==============================] - 0s 120us/sample - loss: 0.5313 - acc: 0.7749\n",
      "Epoch 93/100\n",
      "342/342 [==============================] - 0s 99us/sample - loss: 0.5140 - acc: 0.7807\n",
      "Epoch 94/100\n",
      "342/342 [==============================] - 0s 324us/sample - loss: 0.5204 - acc: 0.7398\n",
      "Epoch 95/100\n",
      "342/342 [==============================] - 0s 114us/sample - loss: 0.5175 - acc: 0.7602\n",
      "Epoch 96/100\n",
      "342/342 [==============================] - 0s 111us/sample - loss: 0.5360 - acc: 0.7339\n",
      "Epoch 97/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5008 - acc: 0.7602\n",
      "Epoch 98/100\n",
      "342/342 [==============================] - 0s 122us/sample - loss: 0.5263 - acc: 0.7544\n",
      "Epoch 99/100\n",
      "342/342 [==============================] - 0s 137us/sample - loss: 0.5160 - acc: 0.7427\n",
      "Epoch 100/100\n",
      "342/342 [==============================] - 0s 108us/sample - loss: 0.5256 - acc: 0.7573\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.4901 - acc: 0.7661\n",
      "Baseline Accuracy: 73.49% (3.25%)\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=grid_result.best_params_['epochs'], batch_size=grid_result.best_params_['batch_size'],optimizer=grid_result.best_params_['optimizer'],init=grid_result.best_params_['init'])\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "513/513 [==============================] - 1s 1ms/sample - loss: 0.6717 - acc: 0.5965\n",
      "Epoch 2/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.6555 - acc: 0.6394\n",
      "Epoch 3/100\n",
      "513/513 [==============================] - 0s 103us/sample - loss: 0.6555 - acc: 0.6316\n",
      "Epoch 4/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.6474 - acc: 0.6706\n",
      "Epoch 5/100\n",
      "513/513 [==============================] - 0s 222us/sample - loss: 0.6378 - acc: 0.6647\n",
      "Epoch 6/100\n",
      "513/513 [==============================] - 0s 108us/sample - loss: 0.6299 - acc: 0.6491\n",
      "Epoch 7/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.6290 - acc: 0.6550\n",
      "Epoch 8/100\n",
      "513/513 [==============================] - 0s 94us/sample - loss: 0.6265 - acc: 0.6569\n",
      "Epoch 9/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.6313 - acc: 0.6589\n",
      "Epoch 10/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.6207 - acc: 0.6667\n",
      "Epoch 11/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.6313 - acc: 0.6530\n",
      "Epoch 12/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.6302 - acc: 0.6608\n",
      "Epoch 13/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.6256 - acc: 0.6413\n",
      "Epoch 14/100\n",
      "513/513 [==============================] - 0s 106us/sample - loss: 0.6164 - acc: 0.6706\n",
      "Epoch 15/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.6308 - acc: 0.6550\n",
      "Epoch 16/100\n",
      "513/513 [==============================] - 0s 233us/sample - loss: 0.6207 - acc: 0.6784\n",
      "Epoch 17/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.6161 - acc: 0.6667\n",
      "Epoch 18/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6305 - acc: 0.6686\n",
      "Epoch 19/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6203 - acc: 0.6725\n",
      "Epoch 20/100\n",
      "513/513 [==============================] - 0s 103us/sample - loss: 0.6111 - acc: 0.6842\n",
      "Epoch 21/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.6125 - acc: 0.6745\n",
      "Epoch 22/100\n",
      "513/513 [==============================] - 0s 111us/sample - loss: 0.6082 - acc: 0.6784\n",
      "Epoch 23/100\n",
      "513/513 [==============================] - 0s 245us/sample - loss: 0.5951 - acc: 0.6764\n",
      "Epoch 24/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.6255 - acc: 0.6647\n",
      "Epoch 25/100\n",
      "513/513 [==============================] - 0s 96us/sample - loss: 0.6114 - acc: 0.6745\n",
      "Epoch 26/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6002 - acc: 0.6725\n",
      "Epoch 27/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.6091 - acc: 0.6764\n",
      "Epoch 28/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6149 - acc: 0.6920\n",
      "Epoch 29/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.6002 - acc: 0.6862\n",
      "Epoch 30/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5903 - acc: 0.6881\n",
      "Epoch 31/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.6112 - acc: 0.6628\n",
      "Epoch 32/100\n",
      "513/513 [==============================] - 0s 103us/sample - loss: 0.6132 - acc: 0.6667\n",
      "Epoch 33/100\n",
      "513/513 [==============================] - 0s 241us/sample - loss: 0.6130 - acc: 0.6628\n",
      "Epoch 34/100\n",
      "513/513 [==============================] - 0s 117us/sample - loss: 0.6038 - acc: 0.6647\n",
      "Epoch 35/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5983 - acc: 0.6784\n",
      "Epoch 36/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6006 - acc: 0.6764\n",
      "Epoch 37/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5910 - acc: 0.6901\n",
      "Epoch 38/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.6056 - acc: 0.6920\n",
      "Epoch 39/100\n",
      "513/513 [==============================] - 0s 111us/sample - loss: 0.5943 - acc: 0.6842\n",
      "Epoch 40/100\n",
      "513/513 [==============================] - 0s 257us/sample - loss: 0.5922 - acc: 0.6862\n",
      "Epoch 41/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.5943 - acc: 0.6784\n",
      "Epoch 42/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.6036 - acc: 0.6667\n",
      "Epoch 43/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.6004 - acc: 0.6881\n",
      "Epoch 44/100\n",
      "513/513 [==============================] - 0s 114us/sample - loss: 0.6215 - acc: 0.6784\n",
      "Epoch 45/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5807 - acc: 0.6823\n",
      "Epoch 46/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5818 - acc: 0.6803\n",
      "Epoch 47/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5943 - acc: 0.6862\n",
      "Epoch 48/100\n",
      "513/513 [==============================] - 0s 115us/sample - loss: 0.6063 - acc: 0.6959\n",
      "Epoch 49/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.6021 - acc: 0.6784\n",
      "Epoch 50/100\n",
      "513/513 [==============================] - 0s 260us/sample - loss: 0.5830 - acc: 0.7057\n",
      "Epoch 51/100\n",
      "513/513 [==============================] - 0s 110us/sample - loss: 0.5758 - acc: 0.6803\n",
      "Epoch 52/100\n",
      "513/513 [==============================] - 0s 100us/sample - loss: 0.5892 - acc: 0.6901\n",
      "Epoch 53/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.5877 - acc: 0.6998\n",
      "Epoch 54/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5739 - acc: 0.7096\n",
      "Epoch 55/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5841 - acc: 0.6823\n",
      "Epoch 56/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5930 - acc: 0.6901\n",
      "Epoch 57/100\n",
      "513/513 [==============================] - 0s 243us/sample - loss: 0.5893 - acc: 0.6842\n",
      "Epoch 58/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5843 - acc: 0.7096\n",
      "Epoch 59/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5834 - acc: 0.7135\n",
      "Epoch 60/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5832 - acc: 0.6940\n",
      "Epoch 61/100\n",
      "513/513 [==============================] - 0s 94us/sample - loss: 0.5844 - acc: 0.6842\n",
      "Epoch 62/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5839 - acc: 0.6881\n",
      "Epoch 63/100\n",
      "513/513 [==============================] - 0s 113us/sample - loss: 0.5945 - acc: 0.7076\n",
      "Epoch 64/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5720 - acc: 0.7076\n",
      "Epoch 65/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5789 - acc: 0.7018\n",
      "Epoch 66/100\n",
      "513/513 [==============================] - 0s 107us/sample - loss: 0.5834 - acc: 0.7212\n",
      "Epoch 67/100\n",
      "513/513 [==============================] - 0s 109us/sample - loss: 0.5614 - acc: 0.7251\n",
      "Epoch 68/100\n",
      "513/513 [==============================] - 0s 232us/sample - loss: 0.5817 - acc: 0.7096\n",
      "Epoch 69/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5814 - acc: 0.6842\n",
      "Epoch 70/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5762 - acc: 0.7212\n",
      "Epoch 71/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5674 - acc: 0.7076\n",
      "Epoch 72/100\n",
      "513/513 [==============================] - 0s 98us/sample - loss: 0.5648 - acc: 0.7135\n",
      "Epoch 73/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5877 - acc: 0.6998\n",
      "Epoch 74/100\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.5616 - acc: 0.7096\n",
      "Epoch 75/100\n",
      "513/513 [==============================] - 0s 251us/sample - loss: 0.5768 - acc: 0.7037\n",
      "Epoch 76/100\n",
      "513/513 [==============================] - 0s 115us/sample - loss: 0.5750 - acc: 0.7076\n",
      "Epoch 77/100\n",
      "513/513 [==============================] - 0s 108us/sample - loss: 0.5751 - acc: 0.7115\n",
      "Epoch 78/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5778 - acc: 0.7193\n",
      "Epoch 79/100\n",
      "513/513 [==============================] - 0s 102us/sample - loss: 0.5666 - acc: 0.7018\n",
      "Epoch 80/100\n",
      "513/513 [==============================] - 0s 111us/sample - loss: 0.5669 - acc: 0.7271\n",
      "Epoch 81/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5655 - acc: 0.7135\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5636 - acc: 0.7115\n",
      "Epoch 83/100\n",
      "513/513 [==============================] - 0s 99us/sample - loss: 0.5595 - acc: 0.7173\n",
      "Epoch 84/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5902 - acc: 0.6901\n",
      "Epoch 85/100\n",
      "513/513 [==============================] - 0s 247us/sample - loss: 0.5678 - acc: 0.7154\n",
      "Epoch 86/100\n",
      "513/513 [==============================] - 0s 94us/sample - loss: 0.5776 - acc: 0.7076\n",
      "Epoch 87/100\n",
      "513/513 [==============================] - 0s 91us/sample - loss: 0.5717 - acc: 0.7349\n",
      "Epoch 88/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5768 - acc: 0.6842\n",
      "Epoch 89/100\n",
      "513/513 [==============================] - 0s 100us/sample - loss: 0.5732 - acc: 0.7154\n",
      "Epoch 90/100\n",
      "513/513 [==============================] - 0s 93us/sample - loss: 0.5531 - acc: 0.7310\n",
      "Epoch 91/100\n",
      "513/513 [==============================] - 0s 89us/sample - loss: 0.5692 - acc: 0.7076\n",
      "Epoch 92/100\n",
      "513/513 [==============================] - 0s 94us/sample - loss: 0.5636 - acc: 0.7173\n",
      "Epoch 93/100\n",
      "513/513 [==============================] - 0s 258us/sample - loss: 0.5697 - acc: 0.7173\n",
      "Epoch 94/100\n",
      "513/513 [==============================] - 0s 105us/sample - loss: 0.5759 - acc: 0.7173\n",
      "Epoch 95/100\n",
      "513/513 [==============================] - 0s 115us/sample - loss: 0.5720 - acc: 0.6998\n",
      "Epoch 96/100\n",
      "513/513 [==============================] - 0s 95us/sample - loss: 0.5706 - acc: 0.7173\n",
      "Epoch 97/100\n",
      "513/513 [==============================] - 0s 98us/sample - loss: 0.5696 - acc: 0.7271\n",
      "Epoch 98/100\n",
      "513/513 [==============================] - 0s 97us/sample - loss: 0.5622 - acc: 0.7076\n",
      "Epoch 99/100\n",
      "513/513 [==============================] - 0s 101us/sample - loss: 0.5644 - acc: 0.7271\n",
      "Epoch 100/100\n",
      "513/513 [==============================] - 0s 89us/sample - loss: 0.5704 - acc: 0.6998\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        75\n",
      "           1       0.75      0.39      0.51        54\n",
      "\n",
      "    accuracy                           0.69       129\n",
      "   macro avg       0.71      0.65      0.64       129\n",
      "weighted avg       0.71      0.69      0.66       129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.91      0.76        75\n",
      "           1       0.72      0.33      0.46        54\n",
      "\n",
      "    accuracy                           0.67       129\n",
      "   macro avg       0.69      0.62      0.61       129\n",
      "weighted avg       0.68      0.67      0.63       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_predictions))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68  7]\n",
      " [33 21]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
